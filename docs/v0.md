# v0 — Protocol Specification

v0 is the bootstrap protocol. It is the only version designed by hand — everything after is proposed and ratified by the network through v0's own mechanisms.

## Table of Contents

1. [Identity](#1-identity)
2. [Message Format](#2-message-format)
3. [Transport](#3-transport)
4. [Peer Discovery](#4-peer-discovery)
5. [Gossip](#5-gossip)
6. [Content](#6-content)
7. [Proposals](#7-proposals)
8. [Votes](#8-votes)
9. [Reputation](#9-reputation)
10. [Sybil Resistance](#10-sybil-resistance)
11. [Anti-Gaming](#11-anti-gaming)
12. [Partition Detection](#12-partition-detection)
13. [Protocol Evolution](#13-protocol-evolution)
14. [Error Handling](#14-error-handling)
15. [Open Questions](#15-open-questions)

---

## 1. Identity

**Terminology:** A *node* is a running instance with a keypair. An *identity* is a root key plus any authorized children (§1 Identity Linking). A *peer* is any node you're connected to. After identity linking is introduced, 'node' and 'identity' diverge — one identity may span multiple nodes.

A node is an Ed25519 keypair.

- **Private key:** 32 bytes, Ed25519. Never transmitted.
- **Public key:** 32 bytes, Ed25519. Serves as the node's identity.
- **Node ID:** The hex-encoded public key. This is the canonical identifier.

All messages are signed by the sender's private key. All signatures are verified against the sender's public key.

### Key Operations

| Operation | Algorithm | Input | Output |
|---|---|---|---|
| Sign | Ed25519 | message bytes + private key | 64-byte signature |
| Verify | Ed25519 | message bytes + signature + public key | boolean |

### Key Rotation

A node MAY rotate its keypair by publishing a `KEY_ROTATE` message signed by **both** the old and new keys:

```json
{
  "type": "KEY_ROTATE",
  "payload": {
    "old_key": "<hex_old_public_key>",
    "new_key": "<hex_new_public_key>",
    "new_key_signature": "<hex — payload signed by new private key>"
  }
}
```

The envelope itself is signed by the old key (standard signing). The `new_key_signature` field contains the same payload signed by the new private key. Nodes that verify both signatures MUST update their peer table to associate the old key's reputation with the new key.

**Only one rotation per key:** Nodes MUST accept only the **first** KEY_ROTATE message seen for a given `old_key`. Subsequent KEY_ROTATE messages with the same `old_key` but a different `new_key` MUST be rejected. This prevents reputation cloning via race conditions during gossip propagation.

After rotation, messages signed by the old key MUST be rejected after a **1-hour** grace period to allow propagation. The 1-hour grace period is measured from the KEY_ROTATE message's `timestamp` field (not local receipt time): messages from the old key MUST be rejected after `KEY_ROTATE.timestamp + 3,600,000 ms`. All nodes use the message timestamp for deterministic cutoff. One hour is conservative — gossip convergence in a healthy network is ~15 minutes. The grace period exists for degraded conditions, not normal operation.

When a partition heals and conflicting KEY_ROTATE messages are discovered for the same `old_key` (different `new_key` values), both new keys MUST be suspended — reputation frozen at floor (0.1), voting weight reduced to 10% of normal (not zeroed, to allow participation in resolution governance) — until the conflict is resolved via a standard governance proposal that determines the legitimate successor. The protocol adds a KEY_CONFLICT message type for nodes to broadcast discovered conflicts:

```json
{
  "type": "KEY_CONFLICT",
  "payload": {
    "old_key": "<hex_public_key>",
    "rotate_message_1_id": "<message_id>",
    "rotate_message_2_id": "<message_id>",
    "new_key_1": "<hex_public_key>",
    "new_key_2": "<hex_public_key>"
  }
}
```

KEY_CONFLICT is broadcast on `/valence/peers`. Any node that observes two KEY_ROTATE messages for the same `old_key` with different `new_key` values MUST broadcast a KEY_CONFLICT and apply the suspension locally.

**Tenure continuity:** Key rotation transfers reputation but does NOT reset tenure (§11). The new key inherits the old key's full activity history. Tenure tracks the *identity's* history, not the key's.

### Identity Linking

A node identity is a root Ed25519 keypair. An operator MAY authorize additional keypairs to act on behalf of the same identity. These are **authorized keys** — they sign messages, connect to peers, and participate in the network, but all actions are attributed to the root identity.

Use cases: a local home node (root key, physically secured), a cloud relay (authorized key), multiple agents (each with an authorized key). One operator, one identity, multiple devices.

#### DID_LINK

```json
{
  "type": "DID_LINK",
  "payload": {
    "root_key": "<hex_public_key>",
    "child_key": "<hex_public_key>",
    "child_signature": "<hex — child signs root_key || child_key>",
    "label": "<optional string — human-readable name, e.g. 'cloud-relay-1'>"
  }
}
```

The envelope is signed by the root key (`from` = root_key). The `child_signature` proves the child key holder consents to linking. Both parties must cooperate.

Rules:
- A root key MAY authorize multiple child keys
- A child key MUST belong to at most one identity (one root)
- A child key MUST NOT also be a root key of another identity
- Authorized key sets are flat — one level deep. Hierarchical delegation can be proposed via the protocol.
- First valid DID_LINK for a given child key wins. Subsequent DID_LINK messages claiming the same child for a different root MUST be rejected (same rule as KEY_ROTATE first-seen).
- Children MUST still complete their own VDF proof for peer authentication (§10). Linking does not bypass VDF — each key must independently prove work.

#### Identity Message Persistence

DID_LINK and DID_REVOKE messages are **identity-structural** — they define relationships that must be discoverable for the lifetime of the identity. Unlike ephemeral protocol messages, they are NOT subject to the 24-hour gossip age limit (§5).

**Retention:** Nodes MUST retain all DID_LINK and DID_REVOKE messages indefinitely (or at least for the lifetime of the root identity). These messages MUST be included in sync responses when relevant identity information is requested.

**Periodic re-broadcast:** Root identities SHOULD re-broadcast their DID_LINK messages every **30 days** on `/valence/peers` to ensure new nodes can discover identity relationships. New nodes joining the network rely primarily on the sync protocol (below) for identity discovery; re-broadcast is a secondary mechanism for resilience.

**Sync extension:** The `/valence/sync/1.0.0` protocol MUST support identity-specific queries. When a node encounters a message from an unknown key, it MAY request the identity chain for that key:

```json
{"type": "SYNC_REQUEST", "payload": {"types": ["DID_LINK", "DID_REVOKE"], "identity_key": "<hex — root or child key>", "limit": 100}}
```

Nodes respond with all DID_LINK and DID_REVOKE messages involving the requested key (as root or child). This allows late-joining nodes to reconstruct the full identity graph. **Rate limiting:** Nodes MUST rate-limit SYNC_REQUEST messages to **10 per peer per minute**. Identity-specific queries (with `identity_key` set) count toward this limit. This prevents DoS via exhaustive identity graph enumeration.

Without identity message persistence, a child KEY_ROTATE creates an orphaned key that an attacker could claim via DID_LINK after the original link ages out of gossip.

#### DID_REVOKE

```json
{
  "type": "DID_REVOKE",
  "payload": {
    "root_key": "<hex_public_key>",
    "revoked_key": "<hex_public_key>",
    "reason": "<optional string>",
    "effective_from": "<unix_milliseconds — when the operator believes the key should no longer be trusted>"
  }
}
```

Signed by root key only. Nodes MUST reject DID_REVOKE messages where `revoked_key` is not a child of the signing root. Takes effect immediately on receipt — messages from the revoked key MUST be rejected going forward. The `effective_from` field is informational: it signals when the operator believes compromise occurred, allowing other nodes to treat messages from the revoked key between `effective_from` and the revocation receipt as suspect.

After revocation:
- The revoked key is permanently deauthorized — it cannot be re-linked to any identity
- The revoked key MAY re-register as a new independent identity (new VDF, starting at 0.2) but has no connection to its former root
- Re-registered keys that were previously part of an identity face a **60-day voting cooldown** — they cannot vote on proposals until 60 days after the DID_REVOKE timestamp. This prevents link-then-unlink vote multiplication (linking to earn faster with dampened gains, then unlinking for more votes).
- Revocation is permanent and cannot be undone
- The root key MUST NOT revoke itself. To dissolve a group, revoke each child individually. To handle root compromise, use KEY_ROTATE (§1).

#### One Identity, Multiple Keys — Behavioral Rules

1. **Single reputation with gain dampening.** There is one reputation score per identity (the root). All actions by any authorized key earn or lose reputation for the root. A child key's successful storage challenges, proposal adoptions, and verified claims all credit the root identity. Penalties likewise affect the root.

   **Gain dampening:** To prevent multi-key identities from accumulating reputation disproportionately, reputation gains are dampened by the number of authorized keys with steep diminishing returns:

   ```
   effective_gain = raw_gain / authorized_key_count^0.75
   ```

   Where `authorized_key_count` includes the root key (minimum 1, so single-node identities are unaffected). The exponent 0.75 provides sublinear scaling — more keys earn somewhat faster (rewarding genuine infrastructure contribution) but with sharply diminishing returns:

   | Keys | Dampening | Effective gain rate vs. single node |
   |------|-----------|-------------------------------------|
   | 1    | 1.000     | 1.00× (baseline)                   |
   | 2    | 0.595     | 1.19×                               |
   | 4    | 0.354     | 1.41×                               |
   | 10   | 0.178     | 1.78×                               |
   | 100  | 0.032     | 3.16×                               |

   In practice, velocity limits (§9: 0.02/day) cap the advantage further — identities with 10+ keys typically hit the daily velocity cap, limiting real-world advantage to ~2-3× regardless of key count. Penalties are NOT dampened — they apply at full rate regardless of key count. The dampening exponent (0.75) is a protocol constant and can be adjusted via constitutional proposal.

   **Asymmetric risk:** Multi-key identities have asymmetric exposure: gains are dampened by `1/N^0.75` but penalties apply at full rate across all keys. An identity with 100 authorized keys has 100× the penalty surface (any compromised key can trigger penalties) but earns only ~3× faster. Operators MUST weigh this carefully before linking — linking is a security commitment, not a scaling strategy.

2. **Single vote per proposal.** All keys in an identity share one vote per proposal. If a child key votes on a proposal, the vote is attributed to the root identity. A subsequent vote from any key in the same identity supersedes the previous one (same as the normal vote supersession rule in §8 — latest vote wins).

3. **Collusion detection exemption.** Correlated activity between keys in the same identity is expected and MUST NOT trigger collusion penalties (§11). VDF timing correlation between keys in the same identity MUST also be exempted.

4. **Single proposal rate limit.** The 3-proposals-per-7-days limit (§7) applies to the identity as a whole — across all authorized keys.

5. **Shared tenure.** Activity by any authorized key counts toward the identity's tenure (§11).

6. **Shared inactivity.** The identity is "active" (§9) if ANY authorized key has broadcast a signed protocol message within the inactivity window. One active key keeps the whole identity alive.

7. **ASN diversity.** For ASN diversity calculations (§4), an identity counts as one operator regardless of how many keys it has across different ASNs. The network still benefits from the infrastructure — more nodes across more ASNs improves connectivity — but the identity doesn't get outsized diversity credit.

8. **Quorum and headcount.** For minimum voter counts (§8) and cold start headcount voting, an identity counts as ONE voter regardless of how many authorized keys it has.

#### Why Linking Is a Constraint, Not a Benefit

Linking gives up vote multiplication and proposal rate amplification in exchange for not being flagged as a Sybil. An unlinked operator with 3 nodes can cast 3 votes and submit 9 proposals per week — but risks collusion detection. A linked operator casts 1 vote and submits 3 proposals — but is safe from false-positive Sybil flags. Linking is self-policing.

#### Handling Compromise Retroactively

When a child key is compromised and later revoked:
- **Votes:** The root can supersede any votes cast by the compromised key (§8 already allows vote supersession). Cast a new vote from the root or any other authorized key.
- **Proposals:** The root can WITHDRAW any proposals submitted by the compromised key (§7 — only the original author can withdraw, and the compromised key acted as the root identity).
- **Storage/reputation:** Actions that already earned or lost reputation cannot be undone. The damage is bounded by velocity limits (§9).

#### KEY_ROTATE Interaction

- When a root KEY_ROTATEs, the new root inherits the entire set of authorized keys. Children remain linked to the new root.
- When a child KEY_ROTATEs, it remains authorized under the same root. The root does NOT need to re-sign a DID_LINK — the KEY_ROTATE chain proves continuity. A key that became an authorized key via KEY_ROTATE of an existing child MUST be treated as linked — subsequent DID_LINK messages claiming it for a different root MUST be rejected.
- Proposal rate limits transfer across KEY_ROTATE for both root and child keys (consistent with §1 KEY_ROTATE rules).

### DID Representation (Optional)

Nodes MAY represent their identity as a DID for interoperability:

```
did:valence:<hex-encoded-public-key>
```

The protocol does not require DIDs — the raw public key is sufficient.

---

## 2. Message Format

Every message is a signed JSON envelope.

### Canonicalization

All JSON serialization for signing and hashing MUST follow **RFC 8785 (JSON Canonicalization Scheme — JCS)**:

- Object keys sorted lexicographically by Unicode code point
- No whitespace between tokens
- Numbers serialized per ECMAScript rules (no trailing zeros, no leading plus, etc.)
- Unicode: NFC normalization
- Null values included (not omitted)
- Recursively applied to nested objects

Implementations MUST produce identical byte output for identical logical structures. Failure to canonicalize correctly will cause signature verification failures across implementations.

**Numeric types:** All numeric fields in protocol messages MUST be integers. Floating-point values (reputation scores, etc.) MUST be serialized as integers with fixed precision: **multiply by 10,000 and truncate** (e.g., reputation 0.8 → `8000`, threshold 0.67 → `6700`). This eliminates cross-language float serialization divergence. Truncation to fixed-point integers occurs only on storage and wire transmission. Intermediate calculations MUST use at least 64-bit integer arithmetic. When evaluating the reputation formula (§9), operations proceed left-to-right with truncation only on the final result.

### Wire Format

**Stream protocols (point-to-point):** Length-prefixed JSON.

```
+-------------------+-----------------------------+
| 4 bytes (BE u32)  |  JSON payload (UTF-8)       |
| = payload length  |                             |
+-------------------+-----------------------------+
```

Maximum payload size: 8 MiB. Messages exceeding this MUST be rejected.

**GossipSub (broadcast):** Plain UTF-8 JSON. GossipSub handles message boundaries.

### Envelope Schema

```json
{
  "version": 0,
  "type": "<message_type>",
  "id": "<sha256_hex_of_signing_body>",
  "from": "<hex_public_key>",
  "timestamp": <unix_milliseconds>,
  "payload": { ... },
  "signature": "<hex_ed25519_signature>"
}
```

| Field | Type | Description |
|---|---|---|
| `version` | integer | Protocol version. MUST be `0` for this spec. Not included in signing body (see below). |
| `type` | string | Message type. |
| `id` | string | SHA-256 hex digest of the **signing body** (see below). Content address. |
| `from` | string | Hex-encoded Ed25519 public key of the sender. |
| `timestamp` | integer | Unix time in milliseconds when the message was created. |
| `payload` | object | Type-specific payload. |
| `signature` | string | Hex-encoded Ed25519 signature over the signing body bytes. |

### Signing Body

The signature covers the JCS-canonicalized bytes of:

```json
{
  "from": "<from>",
  "payload": <payload>,
  "timestamp": <timestamp>,
  "type": "<type>"
}
```

(Keys in lexicographic order per JCS.)

**Note:** `version` is deliberately excluded from the signing body. This ensures the same logical message has the same `id` and signature regardless of which protocol version it's sent under, preventing message duplication during version transitions.

### Content Addressing

The `id` field is the SHA-256 hex digest of the signing body bytes. Because `id` includes `from` and `timestamp`, two different nodes publishing identical payloads produce different IDs. Messages are deduplicated by `id`.

### Timestamp Validation

Nodes MUST reject messages with timestamps more than **5 minutes** from the node's local time (past or future). This requires loose clock synchronization (NTP). Messages that fail timestamp validation MUST NOT be propagated.

**Clock synchronization:** Nodes MUST attempt NTP synchronization on startup and SHOULD periodically re-sync (suggested: every 6 hours). Two nodes both within ±5 minutes of true time can disagree by up to 10 minutes. To detect drift, nodes MUST compute median timestamp offset from the last 50 received messages. Nodes with >2 minutes offset from the peer-estimated median MUST log a warning and attempt clock correction. Nodes with >3 minutes offset from the peer-estimated median SHOULD be treated as unreliable by peers (reduce reputation weight in local calculations, do not use as sync source).

### Message Type Inventory

| Type | Transport | Section |
|---|---|---|
| AUTH_CHALLENGE | stream `/valence/auth/1.0.0` | §3 |
| AUTH_RESPONSE | stream `/valence/auth/1.0.0` | §3 |
| PEER_ANNOUNCE | GossipSub `/valence/peers` | §4 |
| PEER_LIST_REQUEST | stream `/valence/sync/1.0.0` | §4 |
| PEER_LIST_RESPONSE | stream `/valence/sync/1.0.0` | §4 |
| SYNC_REQUEST | stream `/valence/sync/1.0.0` | §5 |
| SYNC_RESPONSE | stream `/valence/sync/1.0.0` | §5 |
| REQUEST | GossipSub `/valence/proposals` | §7 |
| PROPOSE | GossipSub `/valence/proposals` | §7 |
| WITHDRAW | GossipSub `/valence/proposals` | §7 |
| ADOPT | GossipSub `/valence/proposals` | §7 |
| VOTE | GossipSub `/valence/votes` | §8 |
| REPUTATION_GOSSIP | GossipSub `/valence/peers` | §9 |
| STORAGE_CHALLENGE | stream `/valence/sync/1.0.0` | §6 |
| STORAGE_PROOF | stream `/valence/sync/1.0.0` | §6 |
| SHARD_QUERY | stream `/valence/sync/1.0.0` | §6 |
| SHARD_QUERY_RESPONSE | stream `/valence/sync/1.0.0` | §6 |
| KEY_ROTATE | GossipSub `/valence/peers` | §1 |
| KEY_CONFLICT | GossipSub `/valence/peers` | §1 |
| DID_LINK | GossipSub `/valence/peers` | §1 |
| DID_REVOKE | GossipSub `/valence/peers` | §1 |
| CHALLENGE_RESULT | GossipSub `/valence/proposals` | §6 |
| SHARE | GossipSub `/valence/peers` | §6 |
| REPLICATE_REQUEST | GossipSub `/valence/proposals` | §6 |
| REPLICATE_ACCEPT | GossipSub `/valence/proposals` | §6 |
| SHARD_ASSIGNMENT | GossipSub `/valence/proposals` | §6 |
| SHARD_RECEIVED | GossipSub `/valence/proposals` | §6 |
| CONTENT_REQUEST | stream `/valence/content/1.0.0` | §6 |
| CONTENT_RESPONSE | stream `/valence/content/1.0.0` | §6 |
| FLAG | GossipSub `/valence/peers` | §6 |
| COMMENT | GossipSub `/valence/proposals` | §7 |
| RENT_PAYMENT | GossipSub `/valence/proposals` | §6 |
| CONTENT_WITHDRAW | GossipSub `/valence/proposals` | §6 |
| STATE_SNAPSHOT | stream `/valence/sync/1.0.0` | §5 |

Stream protocol messages (AUTH_*, PEER_LIST_*, SYNC_*, STORAGE_*, SHARD_*) use the envelope format from §2. AUTH_CHALLENGE and AUTH_RESPONSE are exceptions — they use a simplified schema defined in §3 since they occur before identity is established.

---

## 3. Transport

v0 uses libp2p as the transport layer.

### Stream Protocols (Point-to-Point)

| Protocol ID | Purpose |
|---|---|
| `/valence/sync/1.0.0` | Pull-based synchronization and peer exchange |
| `/valence/auth/1.0.0` | Authentication handshake (see below) |
| `/valence/content/1.0.0` | Direct content transfer (hosted content download) |

### Authentication Handshake

When two nodes first connect, they exchange an auth challenge over `/valence/auth/1.0.0`:

1. **Initiator** sends `AUTH_CHALLENGE` with a random 32-byte nonce and their own public key
2. **Responder** signs `nonce || initiator_public_key` with their private key and returns `AUTH_RESPONSE`
3. **Initiator** verifies signature (over nonce + own key), VDF proof, and adds peer to table

```json
{"type": "AUTH_CHALLENGE", "payload": {"nonce": "<hex_32_random_bytes>", "initiator_key": "<hex_public_key>"}}
{"type": "AUTH_RESPONSE", "payload": {"signature": "<hex — signs nonce||initiator_key>", "public_key": "<hex>", "vdf_proof": { ... }}}
```

Binding the initiator's key into the signed response prevents replay attacks — an AUTH_RESPONSE is only valid for the specific initiator that issued the challenge.

Nodes MUST complete auth before accepting any other messages from a peer.

### GossipSub Topics (Broadcast)

| Topic | Purpose |
|---|---|
| `/valence/proposals` | New proposals, edits, withdrawals |
| `/valence/votes` | Vote broadcasts |
| `/valence/peers` | Peer discovery announcements |

Reference implementations SHOULD use these GossipSub parameters: D=6, D_low=4, D_high=12, fanout=6, heartbeat=1s, message_cache_ttl=3 heartbeats.

### NAT Traversal

Nodes SHOULD support libp2p circuit relay v2 for NAT traversal. Nodes with public addresses SHOULD act as relay nodes.

### Connection Security

All connections use libp2p's Noise protocol for encryption and authentication.

---

## 4. Peer Discovery

### Bootstrap

A new node starts with at least one known peer address (a bootstrap node). Bootstrap nodes are regular nodes that happen to be well-known.

Reference implementations MUST include a default bootstrap list. Nodes MAY configure additional or alternative bootstrap nodes.

### Fallback Discovery

When GossipSub-based discovery is unavailable (no connected peers), nodes SHOULD attempt:

1. **mDNS** — discover peers on the local network (for development and LAN deployments)
2. **DNS TXT records** — query a well-known domain for bootstrap peer multiaddrs (e.g., `_valence-bootstrap._tcp.valence.network`)

### Peer Exchange via Sync Protocol

The `/valence/sync/1.0.0` protocol MUST support direct peer list exchange, independent of GossipSub:

```json
{"type": "PEER_LIST_REQUEST", "payload": {"limit": 50, "after": "<node_id | null>"}}
{"type": "PEER_LIST_RESPONSE", "payload": {"peers": [{"node_id": "<hex>", "addresses": ["<multiaddr>"]}], "has_more": true}}
```

Peers are sorted lexicographically by `node_id`. Use `after` for pagination.

This enables a freshly bootstrapped node to discover peers before joining the GossipSub mesh.

### Peer Announcements

Nodes announce on the `/valence/peers` GossipSub topic:

```json
{
  "type": "PEER_ANNOUNCE",
  "payload": {
    "addresses": ["<multiaddr>", ...],
    "capabilities": ["propose", "vote", "store"],
    "version": 0,
    "uptime_seconds": <integer>,
    "vdf_proof": { ... }
  }
}
```

**Capability vocabulary:** `"propose"` (can author proposals), `"vote"` (can cast votes), `"store"` (can hold shards and participate in storage challenges). Unknown capabilities SHOULD be accepted and ignored (forward compatibility for capability strings proposed via governance).

Nodes SHOULD announce every **5 minutes**. Nodes SHOULD prune peers that haven't announced in **30 minutes**.

### Anti-Fragmentation

Nodes MUST periodically connect to random peers outside their immediate neighborhood (suggested: every **10 minutes**). This prevents the network from clustering into disconnected subgraphs.

### ASN Diversity

Nodes MUST track the Autonomous System Number (ASN) distribution of their peers and enforce:

- No single ASN > 25% of connections
- Minimum 4 distinct ASNs (when enough peers are available)

This provides eclipse resistance — an attacker controlling one network segment cannot dominate a node's peer set. ASN diversity is MUST (not SHOULD) because without it, eclipse attacks are trivially cheap (20 nodes across 4 ASNs).

---

## 5. Gossip

### Push

When a node creates or receives a new message, it publishes to the appropriate GossipSub topic.

### Pull (Sync)

Nodes periodically request missed messages from peers via `/valence/sync/1.0.0`:

**Sync Request:**

```json
{
  "type": "SYNC_REQUEST",
  "payload": {
    "since_timestamp": <unix_milliseconds>,
    "since_id": "<message_id — for cursor stability>",
    "types": ["PROPOSE", "VOTE"],
    "limit": 100
  }
}
```

**Sync Response:**

```json
{
  "type": "SYNC_RESPONSE",
  "payload": {
    "messages": [ ... ],
    "has_more": true,
    "next_timestamp": <unix_milliseconds>,
    "next_id": "<message_id>",
    "checkpoint": "<merkle_root_hex>"
  }
}
```

Pagination uses a `(timestamp, id)` cursor to handle multiple messages at the same millisecond deterministically. Sort order: ascending by `timestamp`, then ascending lexicographic by `id`. Messages with `timestamp` > `since_timestamp`, OR (`timestamp` == `since_timestamp` AND `id` > `since_id`), are included.

### State Reconciliation

When a node joins the network for the first time, or reconnects after being offline, it MUST perform state reconciliation before participating fully. A node that acts on stale state — accepting messages from revoked keys, evaluating proposals with outdated reputation, or missing content that was published during its absence — degrades network integrity.

#### Sync Status

Nodes MUST include a `sync_status` field in PEER_ANNOUNCE messages: `"syncing"`, `"degraded"`, or `"synced"`. Nodes with status `"syncing"` or `"degraded"` MUST NOT be selected as sync peers by other nodes — this prevents sync-from-stale-peer cascades where a syncing node serves incomplete state to another syncing node.

Peers MUST NOT trigger partition detection (§12) based on Merkle divergence with nodes whose `sync_status` is `"syncing"` — newly joining nodes will naturally diverge until sync completes. When two `"synced"` nodes detect identity Merkle divergence, this follows the partition detection path (§12), not the sync path — both nodes have committed state and the divergence indicates a genuine network split that healed.

#### Sync Phases

Reconciliation proceeds in priority order. Phases 1, 2, and 3 MUST complete in that order. Phases 4 and 5 MAY proceed in parallel after phase 3 completes. During sync, the node MUST NOT cast votes, create proposals, or issue flags.

**Gossip buffering during sync:** Live gossip messages received during sync MUST be classified by their phase (using the message type → phase mapping in the table below). Messages for **already-completed phases** are applied immediately (they can be validated against committed state). Messages for the **current or future phases** are buffered and applied in timestamp order when that phase completes. The gossip buffer MUST be bounded to **100,000 messages or 100 MiB** (whichever is reached first) per phase. When the buffer fills, messages are evicted by priority: lower-priority types are dropped first (within the same priority, oldest first). Priority order per phase: phase 1: `DID_REVOKE` > `KEY_ROTATE` > `DID_LINK`; phase 3: `VOTE` > `PROPOSE` > `COMMENT`; other phases: all types equal priority (oldest first). The node MUST track the timestamp of the oldest dropped message and use it as `since_timestamp` for a follow-up incremental sync pass after the phase completes, ensuring the dropped range is re-fetched.

| Phase | Message Types | Why First |
|-------|--------------|-----------|
| 1. Identity | `DID_LINK`, `DID_REVOKE`, `KEY_ROTATE` | Revocations affect all subsequent validation — a node must know which keys are valid before processing anything else |
| 2. Reputation | `REPUTATION_GOSSIP` | Vote weights, capability ramp checks, and rate limits all depend on current reputation state |
| 3. Proposals & Votes | `PROPOSE`, `VOTE`, `COMMENT` | Active governance state, needed for content decisions and to avoid proposing duplicates |
| 4. Content Metadata | `SHARE`, `FLAG`, `CONTENT_WITHDRAW`, `RENT_PAYMENT` | Content lifecycle state — what exists, what's flagged, what's withdrawn |
| 5. Storage State | `REPLICATE_REQUEST`, `REPLICATE_ACCEPT`, `SHARD_ASSIGNMENT`, `SHARD_RECEIVED` | Only relevant for nodes with the `store` capability. Nodes without `store` MUST skip phase 5. |

Each phase uses the existing `SYNC_REQUEST` message with the `types` field set to that phase's message types. The `since_timestamp` MUST be persisted to disk on clean shutdown and loaded on restart. If the persisted timestamp is in the future (corruption), the node MUST fall back to `now - 24 hours`. If no persisted timestamp exists (first join or corruption), use `0`.

#### State Snapshots

For networks with sustained high message volume, replaying all historical messages becomes infeasible. To bound sync time:

- Nodes with reputation ≥ **0.7** SHOULD publish signed state snapshots at least once per **12 hours** and at most once per **6 hours** while synced and online. Snapshots are a compact summary of current identity graph, reputation state, and active proposal set, signed by the publishing node. The 0.7 threshold ensures only well-established nodes can publish, raising the cost of coordinated snapshot poisoning.
- Snapshots are identified by `(publisher_node_id, timestamp)` and include Merkle roots for identity and proposal state.
- Snapshots MUST have a `snapshot_timestamp` within **24 hours** of the current time. Syncing nodes MUST reject snapshots older than 24 hours.
- A syncing node MAY bootstrap from a snapshot if it receives the **same snapshot content (matching Merkle roots) from ≥5 snapshot publishers across ≥3 ASNs**. This matches the sync peer threshold and prevents a small colluding group from poisoning bootstrap.
- **Post-snapshot verification:** After snapshot bootstrap, the node MUST perform Merkle root verification against at least **2 peers that were NOT among the specific publishers whose snapshots were used for bootstrap**. These cross-check peers need not be from the original 5 sync peers — the node MAY connect to additional peers specifically for cross-checking. "Publisher" means a node whose specific snapshot was used in this bootstrap, not any node capable of publishing. When Merkle roots diverge, the node SHOULD perform Merkle tree narrowing to identify specific differences: messages present in cross-check peers but absent from the snapshot are fetched and integrated; messages present in the snapshot but absent from cross-check peers AND unsigned or signed by unknown keys trigger snapshot discard and full phased sync fallback. If signed and verifiable, they are retained (the cross-check peers may be slightly behind). **Small network fallback:** If the node cannot find 2 non-publisher cross-check peers after attempting **10 additional connections**, it MAY proceed with cross-checking against any 2 synced peers not among the 5 whose snapshots were used, even if those peers have published other snapshots at other times.
- After successful snapshot verification, the node performs incremental sync from `snapshot_timestamp - 1 hour` (safety margin against backdated snapshots) to catch up on messages published since.
- The `active_node_count`, `identity_count`, and `proposal_count` fields in snapshots are **advisory only**. After bootstrap, the node MUST compute these values from its own state (count leaves in Merkle trees) rather than trusting snapshot claims. These fields MUST NOT be used for governance calculations (quorum sizing, etc.).
- Snapshot publishing is optional and earns no direct reputation reward. The incentive is indirect: a healthy network with easy onboarding benefits all participants.
- Snapshots MUST be requested via the sync protocol (`/valence/sync/1.0.0`), NOT broadcast on GossipSub. This prevents snapshot traffic from congesting gossip topics. Syncing nodes request snapshots from peers with `sync_status: "synced"` and reputation ≥ 0.7.

**Snapshot message:**
```json
{
  "type": "STATE_SNAPSHOT",
  "payload": {
    "identity_merkle_root": "<hex>",
    "proposal_merkle_root": "<hex>",
    "reputation_summary_hash": "<hex — SHA-256 of sorted (node_id, reputation) pairs>",
    "active_node_count": 1234,
    "snapshot_timestamp": 1234567890000,
    "identity_count": 567,
    "proposal_count": 89
  }
}
```

**REPUTATION_CURRENT:** As an optimization, syncing nodes MAY request current reputation state rather than replaying all historical REPUTATION_GOSSIP messages. Current-state sync uses a new `SYNC_REQUEST` type filter: `"REPUTATION_CURRENT"`. The response contains `(node_id, reputation, last_assessment_timestamp)` tuples, where `last_assessment_timestamp` is the `assessment_timestamp` of the most recent REPUTATION_GOSSIP assessment the responding peer has issued for the subject. Syncing nodes MAY use this to estimate staleness — peers with significantly older timestamps may have stale values.

**Integrity:** REPUTATION_CURRENT is a convenience shortcut, not an authoritative source. The syncing node MUST request it from all 5 sync peers and apply **trimmed minimum** aggregation: for each subject, discard the highest and lowest reported values, then use the **minimum of the remaining 3**. This tolerates one Byzantine peer on each extreme (one deflating, one inflating) while remaining conservative. If the interquartile range (difference between the 2nd-highest and 2nd-lowest values) exceeds **0.1** for any subject, the node MUST fall back to full REPUTATION_GOSSIP replay for that subject — wide disagreement among the middle peers indicates genuine uncertainty that shortcuts cannot resolve. REPUTATION_CURRENT responses are subject to the same α=0 cap as any unverified peer assessment (§9).

**Post-sync cross-check:** After completing REPUTATION_CURRENT sync, the node SHOULD compute a reputation summary hash from the aggregated tuples and compare it against the snapshot's `reputation_summary_hash` (if snapshot bootstrap was used). Divergence SHOULD be logged as a warning — some divergence is expected since reputation is locally computed, but large divergence may indicate snapshot staleness or manipulation.

#### Identity Merkle Tree

In addition to the proposal Merkle tree (§12), nodes MUST maintain an **identity Merkle tree** over all identity-structural messages:

- **Leaves:** SHA-256 hash of each `DID_LINK`, `DID_REVOKE`, and `KEY_ROTATE` message `id`
- **Ordering:** Leaves sorted lexicographically by message `id`
- **Structure:** Same as proposal Merkle tree (binary, left-biased for odd counts)
- **Recomputation:** Nodes SHOULD batch Merkle tree recomputation to at most **once per minute** (not per message). This bounds the computational cost when processing many identity messages in rapid succession.

This tree enables Merkle-based divergence detection for identity state. After phase 1 sync, the node MUST compare its identity Merkle root against sync peers. A DID_REVOKE present in any peer's tree but missing from the syncing node's tree MUST be fetched — revocation is irreversible and takes precedence over any conflicting state.

**Retroactive invalidation:** When a DID_REVOKE is applied — whether during sync, from live gossip after phase 1 completes, or during normal operation — the node MUST scan all committed state for messages from the revoked key with timestamps after the revocation's effective timestamp and invalidate them. This includes votes, proposals, flags, and any other signed messages. Additionally, any buffered gossip messages from the revoked key MUST be discarded at buffer-drain time (implementations MUST check revocation status both when buffering and when applying buffered messages). This ensures that late-arriving revocations are fully enforced regardless of when they are discovered.

**Identity divergence severity (extends §12):** Identity Merkle divergence between two `"synced"` nodes is classified separately from proposal divergence:

| Condition | Severity |
|-----------|----------|
| Divergence involves only `DID_LINK` or `KEY_ROTATE` (additive) | info — will converge via gossip |
| Divergence involves any `DID_REVOKE` (subtractive) | **critical** — security-relevant, must resolve immediately |
| Mixed additive + subtractive | **critical** (subtractive takes precedence) |

Nodes detecting critical identity divergence MUST immediately fetch the missing DID_REVOKE messages and revalidate any messages accepted from the revoked key.

#### Merkle-Based Divergence Detection

After completing phases 1–3, the node MUST compare **both** its identity Merkle root and proposal Merkle root against sync peers. If all roots match, the node is consistent. If roots diverge:

1. Exchange Merkle trees at increasing depth to identify divergent subtrees
2. Request the specific message IDs that differ
3. Fetch missing messages via `SYNC_REQUEST`
4. Recompute the Merkle root and re-compare

For proposals in the Merkle tree, sync responses MUST include **all votes** for divergent proposals — not just the proposal itself. This ensures vote completeness and prevents selective omission of individual votes.

This narrows the sync to only the missing data, avoiding full-state transfer on every reconnection.

#### Sync Peer Selection

Nodes MUST sync from at least **5 peers** selected from at least **3 distinct ASNs**. This prevents a single operator (even one controlling nodes across 2 ASNs) from having a majority of sync peers. For first-join sync (`since_timestamp = 0`), at least one sync peer MUST be a bootstrap node (well-known, high-availability nodes listed in the protocol configuration).

When the same message is received from multiple sync peers, the node uses the **first valid instance** (signature-verified, schema-valid). Conflicting messages from different peers (e.g., different REPUTATION_GOSSIP assessments for the same subject) are all ingested — reputation computation already handles multiple assessors (§9). Reputation assessments received during sync are subject to the same α=0 cap as any unverified peer assessment (§9) — sync does not grant elevated trust.

For identity messages (phase 1), nodes MUST request the same identity chains from all sync peers and verify consistency. A DID_REVOKE present on **any** peer MUST be treated as authoritative (revocation is irreversible). For reputation assessments, significant divergence (>0.1 difference for the same subject across sync peers) MUST be logged and the lower value used as the working estimate until corroborated by live gossip.

#### Backpressure

Sync traffic can spike when many nodes reconnect simultaneously (e.g., after a network partition heals). To prevent sync storms:

- **Jitter:** Nodes MUST add random jitter (uniform distribution) before initiating sync after connection. Base jitter: **0–30 seconds**. If the node detects it was in a partition (Merkle divergence from >20% of connected peers), jitter increases to **0–300 seconds**. Nodes offline longer than 24 hours MUST use jitter of `min(300, offline_seconds / 100)` seconds.
- **Exponential backoff:** If a sync peer responds with rate-limit errors or timeouts, the requesting node MUST back off exponentially (starting at 5 seconds, max 5 minutes).
- **Sync bandwidth cap:** Nodes MUST limit sync traffic to **50% of available bandwidth**, preserving capacity for live gossip. Sync-serving nodes MUST cap sync responses to **10 MiB per peer per minute**.
- **Staggered phases:** Nodes MUST wait **1–5 seconds** (uniform random) between completing one sync phase and starting the next, to spread load across sync peers.
- **Elevated sync rate:** Nodes in initial sync (first join or offline >24h) MAY send up to **50 SYNC_REQUEST messages per peer per minute** (5× normal rate). Sync-serving nodes SHOULD honor this elevated rate for peers with `sync_status: "syncing"`. Normal incremental sync uses the standard rate limit of **10 per peer per minute** (§1).

#### Mid-Sync Failure Recovery

If sync fails mid-phase:

- **Completed phases are committed.** Phase 1 and 2 state persisted after each phase completes. On retry, the node resumes from the last incomplete phase.
- **Stale phase check:** If more than **1 hour** has elapsed since phase 1 completed, the node MUST re-run phase 1 before resuming later phases (identity state may have changed).
- **Peer rotation:** On retry after failure, the node MUST select at least **2 new sync peers** (replacing failed peers while maintaining ASN diversity). This prevents a single malicious peer from repeatedly disrupting sync.
- **Max retry backoff:** After 3 consecutive sync failures, the node enters degraded mode rather than retrying immediately. It MUST wait at least **5 minutes** before the next attempt.

#### Sync Completeness

A node considers itself **sync-complete** when:

1. All required phases have completed (phases 1–4 for non-storage nodes; phases 1–5 for storage nodes). No `has_more: true` responses pending.
2. Its identity Merkle root matches at least **3 of 5** sync peers
3. Its proposal Merkle root matches at least **3 of 5** sync peers
4. It has processed at least one `REPUTATION_GOSSIP` batch from each sync peer

After sync-complete, the node transitions to `sync_status: "synced"` and normal operation: it MAY vote, propose, flag, and participate in storage. The transition MUST be logged.

**Degraded mode:** If a node cannot complete sync after repeated attempts, it MAY operate in **degraded mode** (`sync_status: "degraded"`). Degraded mode behavior:

- MUST NOT vote or propose (lacks reliable state for quorum evaluation)
- MAY relay messages, serve storage challenges, and accumulate reputation via uptime
- MUST continue attempting sync with **mandatory peer rotation** on each attempt
- **Graduated exit (phases 1–3 complete):** If phases 1–3 complete but phases 4–5 fail, the node MAY vote and propose (it has identity, reputation, and governance state) but MUST NOT participate in content flagging or storage decisions. This node uses `sync_status: "degraded"` but is functionally capable for governance.
- **Time-limited exit (phases 1–3 incomplete):** After **7 days** in full degraded mode where phases 1–3 have NOT completed, the node remains unable to vote or propose. There is no automatic governance participation with incomplete identity or reputation state — the risk of confused voting outweighs the cost of temporary exclusion. The node MUST continue retry attempts. If phases 1–2 complete during degraded mode (even if phase 3 hasn't), the node MAY vote with **50% weight** — it has identity and reputation context even if its proposal knowledge is incomplete. The VOTE message itself MUST include the voter's `sync_status` at vote creation time. Other nodes apply `min(self_declared_weight, weight_from_observed_sync_status)` — the 50% reduction applies if EITHER the vote declares degraded OR the node independently observes the voter as degraded via PEER_ANNOUNCE. This handles gossip lag in both directions. During cold start (§8), degraded nodes count toward headcount quorum. Cold start uses headcount (no reputation weighting), but the 50% degraded-mode weight is sync-status-based, not reputation-based, and MUST still apply — a node with incomplete state should not have full governance influence regardless of voting mode.

#### Incremental Sync

During normal operation, nodes SHOULD periodically sync with a random peer (suggested: every **15 minutes**, aligned with reputation gossip interval). Incremental sync uses the same `SYNC_REQUEST` mechanism but with `since_timestamp` set to the highest timestamp received from the previous sync (tracked per phase), with a minimum lookback of **30 minutes** from current time. This adaptive window handles both normal operation and degraded gossip conditions where propagation exceeds 30 minutes.

Incremental sync covers phases 1–4. **Identity messages (phase 1) are included** in every 3rd to 5th incremental sync cycle (randomly selected each time, i.e., approximately once per 45–75 minutes). The jitter prevents attackers from timing revoked-key exploits to land in a deterministic gap between identity sync cycles. Additionally, if the previous cycle's live gossip included any DID_REVOKE messages, the next incremental sync MUST include identity messages regardless of the cycle counter. This event-triggered inclusion ensures revocations propagate quickly when they matter.

#### Sync Serving Incentive

Serving sync requests is a network-health contribution. Sync interactions are counted as uptime activity signals for reputation calculation (§9), subject to anti-farming limits: **at most 1 counted interaction per peer per 15 minutes**, and only for sync responses that return **non-empty** `messages` arrays. Counted interactions MUST come from **≥3 distinct peers** in a rolling 24-hour window to qualify for uptime credit. This prevents reciprocal empty-request farming between colluding nodes.

Sync serving quality is tracked **locally by each peer**. Each node tracks which peers responded to its sync requests. Peers that fail to respond to sync requests (timeout or error) are deprioritized in future peer selection — both for sync and for general gossip routing. This peer-local tracking is simpler and more robust than global measurement of refusal rates. **Known limitation:** targeted sync refusal (serving everyone except specific victims) is invisible to the network under local-only tracking. Network-level reputation reporting for sync quality could address this in future versions but is out of scope for v0.

**Post-sync reputation bootstrapping:** After snapshot bootstrap or REPUTATION_CURRENT sync, the node has reputation values for all peers but its direct observation count (α) is zero (§9). This means peer-informed reputation is capped at 0.2 until 3 independent assessors from distinct ASNs are observed. REPUTATION_CURRENT values serve as an upper bound that will be realized as the node accumulates direct observations over the following hours. Initial governance participation quality is limited during this bootstrapping period — this is the intended security behavior, not a bug.

### Deduplication

Nodes maintain a set of seen message IDs. A message MUST NOT be re-propagated if its `id` has already been seen. Implementations SHOULD use a bounded LRU cache (suggested: **100,000 entries**).

**Time-based rejection (gossip only):** Messages received via GossipSub with timestamps older than **24 hours** MUST be rejected. This prevents replay attacks via cache eviction flooding. This rule does NOT apply to messages received via the `/valence/sync/1.0.0` protocol — sync is explicitly for fetching historical messages (proposals with 14-day voting deadlines, etc.).

---

## 6. Content

The network is a general-purpose content layer. Any content type is valid — skills, configs, documents, code, datasets, models, media. The quality and moderation layer determines what persists, not format restrictions.

**Content opacity:** The network treats all content as opaque bytes. Encrypted content is valid at every layer — erasure coding, storage challenges, and shard distribution operate on raw bytes without interpreting content. For hosted and replicated content, encryption is a natural choice for private data; discovery relies on out-of-band key distribution (recipients already know what the content is and how to decrypt it). For proposed content, voters are expected to evaluate what they endorse — opaque or encrypted proposals will naturally fail to reach quorum, as rational voters will reject what they cannot assess. The protocol does not prohibit encrypted proposals; the incentive structure makes them impractical. **Flagging limitation:** Encrypted content is opaque to automated hash-match detection. Enforcement against encrypted illegal content relies on out-of-band reporting — manual flags from nodes that have decrypted the content and can attest to its nature. This is a known limitation by design: the protocol cannot inspect encrypted bytes. Nodes MAY choose not to store encrypted content from low-reputation identities as a local policy.

Content exists in three modes, each a superset of the previous:

### Content Modes

#### Hosted (Self-Served)

A node shares content directly from local storage. The network provides discovery; the node provides availability.

```json
{
  "type": "SHARE",
  "payload": {
    "entries": [
      {
        "content_hash": "<sha256_hex>",
        "content_type": "<mime_type>",
        "content_size": <bytes>,
        "filename": "<optional string>",
        "description": "<optional string>",
        "tags": ["<string — max 64 bytes each, max 20 tags>", ...]
      }
    ]
  }
}
```

SHARE messages are broadcast on `/valence/peers`. Each SHARE message MUST NOT contain more than **50 entries**. Identities are limited to **10 SHARE broadcasts per hour** (covering up to 500 hosted items). Nodes SHOULD re-broadcast SHARE messages every **30 minutes** (consistent with peer expiry). When a node goes offline, its shared content becomes unavailable.

**Properties:**
- No reputation cost (your disk, your bandwidth)
- No durability guarantee — content disappears when you go offline
- Cannot be used in proposals (voters cannot rely on ephemeral content)
- Cannot be voted on
- CAN be flagged (you are distributing it; attribution applies)
- Downloading hosted content uses a direct stream on `/valence/content/1.0.0`:

```json
{"type": "CONTENT_REQUEST", "payload": {"content_hash": "<hex>", "offset": <bytes>, "length": <bytes>}}
{"type": "CONTENT_RESPONSE", "payload": {"content_hash": "<hex>", "offset": <bytes>, "data": "<base64>", "total_size": <bytes>}}
```

Chunked transfer: `length` SHOULD be ≤ 1 MiB per request. Receivers verify the final reassembled content against `content_hash`.

#### Replicated (Network-Stored)

A node requests the network to store content durably using erasure-coded shards. Content survives the uploader going offline.

```json
{
  "type": "REPLICATE_REQUEST",
  "payload": {
    "content_hash": "<sha256_hex>",
    "content_type": "<mime_type>",
    "content_size": <bytes>,
    "filename": "<optional string>",
    "description": "<optional string>",
    "tags": ["<string>", ...],
    "coding": "minimal | standard | resilient",
    "reputation_stake": <fixed_point — reputation committed for storage>
  }
}
```

REPLICATE_REQUEST is broadcast on `/valence/proposals`. The `reputation_stake` is the reputation the uploader commits to pay for network storage (see Storage Economics below).

The `reputation_stake` field is the uploader's committed budget for the first billing cycle's rent. Monthly rent is deducted from the uploader's reputation directly (not drawn from this stake). The stake serves as a signal that the uploader can afford at least one cycle of rent at current pricing.

Storage providers signal willingness to store shards via REPLICATE_ACCEPT:

```json
{
  "type": "REPLICATE_ACCEPT",
  "payload": {
    "content_hash": "<sha256_hex>",
    "shard_indices": [0, 3, 5],
    "capacity_available": <bytes>
  }
}
```

REPLICATE_ACCEPT is broadcast on `/valence/proposals`. The uploader collects acceptances and distributes shards to providers via direct stream on `/valence/content/1.0.0`. If fewer than `data_shards + parity_shards` providers accept within **48 hours**, the replication fails and the `reputation_stake` is returned. Providers MUST have the `store` capability and `available_bytes` ≥ shard size.

After collecting sufficient acceptances, the uploader broadcasts a SHARD_ASSIGNMENT to confirm the distribution:

```json
{
  "type": "SHARD_ASSIGNMENT",
  "payload": {
    "content_hash": "<sha256_hex>",
    "assignments": [
      {"shard_index": 0, "provider": "<hex_node_id>"},
      {"shard_index": 1, "provider": "<hex_node_id>"}
    ]
  }
}
```

SHARD_ASSIGNMENT is broadcast on `/valence/proposals`. Only the identity that submitted the REPLICATE_REQUEST may broadcast it. **Provider confirmation:** Assigned providers MUST broadcast a SHARD_RECEIVED message confirming they hold the shard data before the assignment is considered valid by the network:

```json
{
  "type": "SHARD_RECEIVED",
  "payload": {
    "content_hash": "<sha256_hex>",
    "shard_index": <integer>,
    "shard_hash": "<sha256_hex — hash of received shard data>"
  }
}
```

SHARD_RECEIVED is broadcast on `/valence/proposals`. Nodes SHOULD NOT consider a shard assignment complete until the corresponding SHARD_RECEIVED is observed. Uploaders SHOULD distribute shards across providers from diverse ASNs (minimum 3 distinct ASNs when possible) and SHOULD prefer providers with higher reputation. Providers who broadcast REPLICATE_ACCEPT but are not assigned simply never receive shards — no notification is required. Assigned providers who fail the first storage challenge within 7 days of assignment AND have broadcast SHARD_RECEIVED receive a penalty (**-0.002**) for accept-and-abandon. Providers listed in SHARD_ASSIGNMENT who never broadcast SHARD_RECEIVED are not penalized — the uploader may have failed to deliver. If fewer than `data_shards` providers confirm receipt within 7 days, the replication is considered failed. The uploader MAY reassign shards from non-responsive providers by broadcasting an updated SHARD_ASSIGNMENT.

**Properties:**
- Costs reputation proportional to content size
- Durable — erasure-coded across willing storage peers
- CAN be referenced in proposals
- Storage providers earn reputation via challenges
- Content persists as long as the reputation stake sustains it (see Storage Economics)

**Minimum reputation to replicate:** 0.3. Nodes below this threshold cannot request replication.

#### Content Provenance

When content transitions from hosted to replicated or proposed, the original host MAY be credited via an `origin_share_id` field:

```json
"origin_share_id": "<message_id of the original SHARE message — optional>"
```

This field is valid in both REPLICATE_REQUEST and PROPOSE payloads. When set:

- The referenced SHARE message MUST exist and MUST have a `timestamp` earlier than the REPLICATE_REQUEST or PROPOSE message (proves the host shared it first).
- **Adoption reward split:** The original sharer receives **30%** of adoption rewards; the proposer receives **70%**. The total reputation created is unchanged — provenance redistributes, it does not inflate.
- **Attribution is public.** The `origin_share_id` is visible to all nodes. Voters can see who created the content and who proposed it.
- **Voluntary.** Proposers who do not set `origin_share_id` receive 100% of adoption rewards. The SHARE timestamp is still public — the community can see if content was hosted before it was proposed, and factor that into their evaluation.

Provenance credit is the on-ramp for content creators who haven't reached the 0.3 reputation threshold to propose. A fresh node (0.2) can host useful content, and if an established node proposes it with attribution, the creator earns reputation from adoptions without needing to propose directly.

**Capability gate:** Provenance credit recipients with reputation ≥ 0.3 receive their full share (30%) of adoption rewards. Recipients below 0.3 receive a reduced share capped at **0.002 per proposal** (approximately 1/7th of the normal maximum). This preserves provenance as a functional on-ramp for fresh content creators while limiting farming potential — at 0.002/proposal, reaching 0.3 from 0.2 via provenance alone takes 50 successful proposals, far slower than the legitimate shard storage path.

**Anti-gaming:** Provenance is a reward split, not reward creation. A colluding ring farming provenance credit earns the same total reputation as one without it — the split just moves rewards between ring members. Additionally, provenance recipients below 0.3 receive a reduced rate (0.002/proposal cap), making farming via provenance significantly slower than earning rent through legitimate shard storage. The farming path is slower and more expensive than the legitimate path (storing shards and earning rent).

#### Proposed (Governed)

Replicated content wrapped in a PROPOSE message (§7). Full governance flow — votes, adoption, reputation consequences. Content MUST be replicated before it can be proposed. The proposal references the content by `content_hash`.

### Storage Capacity

Nodes report their storage allocation in PEER_ANNOUNCE:

```json
"storage": {
  "allocated_bytes": <integer>,
  "available_bytes": <integer>,
  "shard_count": <integer>
}
```

This gives the network a supply signal. Nodes choose how much disk to allocate — there is no minimum. Nodes with `"store"` capability and available space are eligible to receive shards.

**Capacity claim verification:** Storage capacity self-reports are unverifiable directly, but nodes SHOULD detect outliers: if a node claims 1 TB allocated but stores < 10 MB of actual shards (as observed via SHARD_QUERY_RESPONSE), the claim is suspicious. Nodes SHOULD weight capacity reports by the reporter's reputation when computing the scarcity multiplier — fresh nodes' capacity claims carry less weight. Additionally, the scarcity multiplier uses a **24-hour moving average** of utilization, preventing sudden manipulation via flash capacity changes.

**Reputation-based storage limits:** The maximum active replicated content per identity scales with reputation:

```
max_active_replicated = base_allowance × (reputation / 0.2) ^ 2
```

Where `base_allowance` = **10 MiB** (the starting allocation at rep 0.2). Examples:
- Rep 0.2: 10 MiB
- Rep 0.3: 22.5 MiB
- Rep 0.5: 62.5 MiB
- Rep 0.8: 160 MiB
- Rep 1.0: 250 MiB

Nodes SHOULD reject REPLICATE_REQUEST messages from identities that would exceed their storage limit.

### Storage Economics

Storage is a **reputation transfer market**. Uploaders pay ongoing rent to storage providers. Reputation is transferred, not created — the total reputation in the system is conserved (minus decay and penalties as the only sinks for healthy behavior).

#### Network Capacity and Pricing

Each node reports `allocated_bytes` and `available_bytes` in PEER_ANNOUNCE. Nodes compute a local **scarcity multiplier** from the aggregate:

```
network_utilization = 1 - (total_available / total_allocated)
scarcity_multiplier = 1 + 99 × network_utilization^4

When `total_allocated = 0` (no nodes report storage capacity), `scarcity_multiplier = 1.0` (no scarcity signal). This is expected during early bootstrap when nodes may not yet offer storage.
```

The quartic curve provides gentle pricing at low utilization and steep pricing as the network fills, with a hard cap at 100×:

Examples:
- 20% utilized: multiplier = 1.0 (essentially free)
- 50% utilized: multiplier = 7.2
- 80% utilized: multiplier = 41.6
- 95% utilized: multiplier = 81.5
- 100% utilized: multiplier = 100.0 (hard cap, no division by zero)

The multiplier is computed locally from each node's view of the network. Slight divergence between nodes is expected and acceptable.

#### Replication Rent

Replicated content has an ongoing monthly cost:

```
monthly_rent = content_size_mib × base_rate × scarcity_multiplier
base_rate = 0.001 per MiB per month
```

Examples at 50% utilization (multiplier 2.0):
- 1 MiB: 0.002/month
- 10 MiB: 0.02/month
- 100 MiB: 0.2/month (significant — requires established reputation)

**Rent price locking with convergence:** The scarcity multiplier used for rent calculation is initially locked at the time of replication. On each subsequent billing cycle, the locked multiplier converges toward the current market multiplier at **10% per cycle**: `effective_multiplier = locked + (current - locked) × min(1, cycles_elapsed / 5)`. After 5 billing cycles (5 months), rent fully reflects the current market rate. When the current multiplier exceeds the locked multiplier by more than 10×, convergence accelerates to 30% per cycle (`cycles_elapsed × 3 / 10`). This handles extreme divergence (e.g., network going from 20% to 90% utilization) without affecting moderate price changes.

**Operational note:** Uploaders should plan for rent at the current market multiplier, not the locked rate. Content replicated during low-utilization periods will see increasing rent as the network grows and convergence progresses. Long-term storage commitments should budget for full market-rate rent.

This prevents both death spirals (sudden rent spikes on existing content) and land grabs (permanently subsidized early-mover storage).

Rent is deducted from the uploader's reputation at the start of each **30-day billing cycle** (computed from the content's replication timestamp). The deducted reputation is distributed to storage providers proportional to shards held (see Storage Challenges below).

**If the uploader cannot pay rent** (insufficient reputation), the content enters a **7-day grace period**. During the grace period, storage providers are not paid but SHOULD continue holding shards. After the grace period, content is eligible for garbage collection — storage nodes MAY drop shards. Seven days is sufficient for a healthy node to earn enough reputation to resume payment (max daily gain 0.02, weekly 0.08) without enabling systematic rent evasion. Storage providers are NOT compensated during grace periods. Providers who continue holding shards do so without payment and may rationally choose to drop shards at grace period entry. Uploaders should not rely on shard survival through grace periods — content may become irretrievable even before the grace period expires. **Repeat failures:** If the same content enters grace period more than once within 90 days, the second grace period is 3 days, and the third is 0 days (immediate GC eligibility).

#### Storage Provider and Validator Revenue

Storage rent is split between **providers** (nodes holding shards) and **validators** (nodes issuing challenges):

- **Providers receive 80%** of rent for content they store, distributed proportional to shards held.
- **Validators receive 20%** of rent, distributed proportional to challenges issued and verified during the billing cycle.

Validators are any node that holds the shard (directly or reconstructed from data shards) and broadcasts a valid CHALLENGE_RESULT. Validating is economically incentivized — nodes earn reputation for keeping providers honest.

**Minimum challenge requirements:** Providers MUST pass at least **10 challenges from at least 2 distinct validators** per billing cycle to receive full rent. Rent is pro-rated by challenges passed: a provider who passes 5 of 10 minimum challenges receives 50% of their provider share. Providers who pass zero challenges receive nothing and pay the failed challenge penalty (-0.01).

**Validator earnings:** Each validated challenge (CHALLENGE_RESULT with `result: pass` or `result: fail`) earns the validator a share of the 20% pool for that content. Validators who discover failures earn **double** their share for that challenge (rewarding honest detection over rubber-stamping). **Validator earnings cap:** Each validator's earnings are capped at **30 challenges per content per billing cycle** (approximately one per day). Challenges beyond this cap still verify storage integrity but do not earn additional validator share. Validator earnings are weighted by unique (validator, provider) pairs challenged, not raw challenge volume — this prevents spam-challenging a single provider from dominating the validator pool.

Validators MUST NOT validate their own shards — CHALLENGE_RESULT messages where the challenger and challenged are the same identity MUST be rejected.

**Collusion resistance:** The ≥2 distinct validators requirement MUST be from ≥2 distinct ASNs. Nodes SHOULD flag validator-provider pairs where one validator accounts for >50% of a provider's passed challenges over 3+ billing cycles. Failed challenges (CHALLENGE_RESULT with `result: fail`) require **corroboration** — a second, independent challenge from a different validator within 24 hours. If the corroborating challenge passes, the original failure report is discounted and the reporting validator's failure rate is noted. Validators whose failure discovery rate is >3× the network average for the same providers SHOULD have their failure reports weighted less in rent calculations.

This three-party model (uploader pays → providers earn most → validators earn some) creates sustainable verification incentives. Without validator rewards, challenge issuance is pure altruism and will be under-provided.

#### Rent Payments

Rent payments are explicit and auditable via the RENT_PAYMENT message:

```json
{
  "type": "RENT_PAYMENT",
  "payload": {
    "content_hash": "<sha256_hex>",
    "billing_cycle": <integer — cycle number from replication timestamp>,
    "amount": <fixed_point — total rent for this cycle>,
    "providers": [
      {"node_id": "<hex>", "shards_held": <integer>, "amount": <fixed_point>}
    ]
  }
}
```

RENT_PAYMENT is broadcast on `/valence/proposals` by the content uploader at the start of each billing cycle. **RENT_PAYMENT is a coordination signal, not an authoritative transfer.** Each node MUST independently compute rent obligations and reputation adjustments from: (a) the REPLICATE_REQUEST's replication timestamp and locked multiplier, (b) the content's shard distribution, (c) which providers passed challenges during the billing cycle, and (d) the convergence formula. Nodes that observe divergence between a RENT_PAYMENT and their own calculation MUST trust their own calculation. The message exists to assist coordination and provide an audit trail, not to dictate reputation changes.

**Timing rules:** RENT_PAYMENT for cycle N MUST be broadcast within the first 7 days of cycle N. Late payments are treated as missed (grace period triggers). Skipped cycles cannot be retroactively paid. Duplicate RENT_PAYMENT messages for the same `(content_hash, billing_cycle)` pair: first-seen wins, subsequent are rejected. Prepayment is not supported.

#### Content Lifecycle

- **Active:** Rent paid, challenges passing, content available. Normal state.
- **Grace period:** Rent unpaid, 30-day countdown. Content still available but providers unpaid.
- **Decayed:** Grace period expired. Storage nodes MAY drop shards. Content may become irretrievable.
- **Proposed content has limited rent exemption.** Content referenced by a proposal is exempt from rent during the **voting period only** (from PROPOSE submission to `voting_deadline`). After the deadline, normal rent applies regardless of outcome. Additionally, the exemption only activates once the proposal receives ≥ **3 endorse votes from distinct identities** — until then, the proposer pays normal rent. This prevents governance spam for free storage (an attacker can propose garbage, but pays rent until 3 real endorsers engage). Adoption rewards for proposed content are split: the proposal author receives **70%** of adoption rewards, and storage providers who held shards during the voting period share the remaining **30%**, proportional to shards held. No additional reputation is created — this is a redistribution of the existing adoption reward pool. (Note: this is independent of the §6 Provenance split — if both apply, provenance splits the author's 70% further into 70/30 with the original host.)

#### Content Withdrawal

An uploader MAY explicitly withdraw replicated content:

```json
{
  "type": "CONTENT_WITHDRAW",
  "payload": {
    "content_hash": "<sha256_hex>",
    "effective_after": "<unix_milliseconds — withdrawal takes effect 24 hours from broadcast>",
    "reason": "<optional string, max 1 KiB>"
  }
}
```

CONTENT_WITHDRAW is broadcast on `/valence/proposals`. Only the identity that currently owns the content may withdraw (see §1 — content ownership follows the identity, not the key; revoked keys have no authority over content attributed to their former root). The `effective_after` field MUST be ≥ 24 hours from the message timestamp, allowing gossip convergence before the withdrawal takes effect. Any PROPOSE referencing the content received before `effective_after` blocks the withdrawal. PROPOSE messages received after a CONTENT_WITHDRAW has been seen MUST be rejected for that content hash.

On receipt, storage providers SHOULD drop shards within 24 hours of `effective_after`. No further rent is charged. Content referenced by ANY active proposal (any proposal whose `voting_deadline` has not yet passed) cannot be withdrawn — the CONTENT_WITHDRAW MUST be rejected until all referencing proposals conclude.

#### Why Transfer, Not Creation

In this model, storage challenges do not create reputation from nothing — they verify that a provider deserves their share of the uploader's rent payment. The only reputation *created* in the system comes from genuinely valuable actions: proposal adoption, verification, and uptime. This prevents inflationary reputation growth and ensures that reputation remains a meaningful signal of contribution.

### Erasure Coding

Large artifacts are distributed using erasure-coded shards across willing peers.

**Erasure coding levels:**

| Level | Data Shards | Parity Shards | Description |
|---|---|---|---|
| minimal | 3 | 2 | 3-of-5 — any 3 shards reconstruct the artifact |
| standard | 5 | 3 | 5-of-8 — default for proposals |
| resilient | 8 | 4 | 8-of-12 — for high-value artifacts |

Protocol change proposals MUST use 'standard' or 'resilient' erasure coding. Nodes SHOULD reject protocol change proposals with 'minimal' coding.

**Flow:**
1. Uploader erasure-codes the artifact into shards
2. Uploader distributes shards to willing peers (nodes with `"store"` capability)
3. Each shard is content-addressed by its own hash
4. The `content_hash` is the hash of the original artifact
5. Nodes wanting the artifact request shards from peers, reconstruct with any `data_shards` count of shards

**Shard metadata** is included in the REPLICATE_REQUEST or proposal:

```json
"content_shards": {
  "coding": "standard",
  "data_shards": 5,
  "parity_shards": 3,
  "shard_hashes": ["<hex>", ...],
  "shard_size": <bytes>,
  "manifest_hash": "<SHA-256 of: shard_count as 4-byte big-endian integer, then shard_hashes sorted lexicographically as hex strings concatenated without delimiters, then content_hash hex string — all as UTF-8 bytes except shard_count which is raw bytes>"
}
```

### Storage Challenges

Storage challenges use adjacency proofs rather than shard retrieval. A challenger picks a random offset within a shard and asks: "What is the hash of the N bytes before/after this offset?" This requires the node to actually hold the shard data — it can't answer by fetching the shard on demand because it doesn't know which offset will be challenged.

```json
{"type": "STORAGE_CHALLENGE", "payload": {"shard_hash": "<hex>", "offset": <bytes>, "direction": "before | after", "window_size": <bytes — default 4096, range 1024..65536>, "challenge_nonce": "<hex — random 32 bytes>"}}
{"type": "STORAGE_PROOF", "payload": {"proof_hash": "<SHA256(challenge_nonce || window_bytes)>"}}
```

The challenger verifies the proof against their own copy of the shard. **Challengers MUST hold the shard** (directly or reconstructed from sufficient data shards) to issue a valid challenge.

**Challenge validation:** After receiving STORAGE_PROOF, the challenger MUST broadcast a signed `CHALLENGE_RESULT` message on `/valence/proposals`:

```json
{"type": "CHALLENGE_RESULT", "payload": {"challenge_id": "<message_id of STORAGE_CHALLENGE>", "shard_hash": "<hex>", "challenged_node": "<hex_public_key>", "result": "pass | fail"}}
```

Third-party nodes track challenge success rates per (challenger, challenged) pair. Suspiciously high pass rates between the same pair (>20 consecutive passes with no failures across all shards) SHOULD trigger collusion review. Challenges MUST be issued randomly by peers who hold the same shard (directly or reconstructed), suggested frequency: once per shard per day. Validators earn a share of storage rent for each verified challenge (see Storage Provider and Validator Revenue above). Nodes MUST NOT issue more than 10 challenges to the same peer per day.

### Shard Discovery

Nodes that store shards announce this via the sync protocol:

```json
{"type": "SHARD_QUERY", "payload": {"content_hash": "<hex>"}}
{"type": "SHARD_QUERY_RESPONSE", "payload": {"available_shards": [0, 3, 5], "shard_hashes": ["<hex>", ...]}}
```

Nodes MUST rate-limit SHARD_QUERY messages to 10 per sender per minute.

### Content Flagging

Any node MAY flag content it believes violates legal or ethical norms:

```json
{
  "type": "FLAG",
  "payload": {
    "content_hash": "<sha256_hex>",
    "severity": "dispute | illegal",
    "category": "dmca | spam | malware | csam | other",
    "details": "<string — evidence or explanation, max 10 KiB>",
    "hash_match": "<optional — identifier of known-bad hash database match, e.g. 'ncmec:2024-xxxxx'>"
  }
}
```

FLAG messages are broadcast on `/valence/peers`. Flagging costs reputation: **-0.005** for `severity: dispute`, **-0.02** for `severity: illegal`. The stake is returned if the flag is upheld by governance vote; forfeited if rejected.

**Automated detection:** Nodes SHOULD run replicated content against known-bad hash databases (e.g., NCMEC/PhotoDNA for CSAM). Automated hash matches generate a FLAG with the `hash_match` field populated. No human review is required or desired — automated hash matching is the primary detection mechanism for severe content violations. **Multi-source requirement:** Automated hash-match flags count toward the shard deletion threshold (above) only when matches come from ≥ **2 independent hash databases** (identified by distinct database prefixes in `hash_match`). A single-database match triggers stop-serving but not shard deletion. This protects against hash database poisoning — a single compromised database cannot cause content destruction. Hash-match flags require the same reputation thresholds as manual flags: ≥ 0.5 for `severity: illegal` (§9 Capability Ramp). Automated detection SHOULD run on established nodes (rep ≥ 0.5) that can meet this threshold.

**Response by severity:**

**Dispute (DMCA, copyright, spam, low-quality):**
- Flags accumulate. At ≥5 flags from ≥3 distinct ASNs, nodes SHOULD deprioritize storage (drop shards when space is needed, stop serving to new requesters).
- A standard governance proposal (§8) can formalize removal. If upheld, the content source's reputation takes -0.05.
- No DID ban. Recoverable.

**Illegal (CSAM, weaponized malware):**
- **Immediate shard deletion** requires ≥ **5 flags** from ≥ **3 distinct ASNs**, OR ≥ **2 independent hash-match flags** (automated detection from different nodes matching different known-bad databases). A single flag — even severity:illegal — does NOT trigger immediate deletion. This prevents Sybil grief-flagging.
- Before the multi-flag threshold is met, nodes that receive a severity:illegal flag SHOULD stop *serving* the flagged content (refuse CONTENT_REQUEST and SHARD_QUERY for that hash) but SHOULD NOT delete shards yet. This limits distribution without destroying evidence.
- Once the threshold is met, nodes SHOULD drop shards within 60 seconds.
- Content source DID is suspended when ≥ 3 severity:illegal flags from ≥ 2 ASNs are received (lower threshold than deletion — suspension is reversible, deletion is not). Suspension: reputation frozen at floor, voting weight reduced to 10%.
- A standard governance proposal for permanent DID ban. If upheld (0.67 threshold), the source DID is permanently banned: all content removed, all reputation forfeited, new messages from the DID MUST be rejected by all nodes.
- Banned DIDs are recorded in a network-wide ban list, included in sync responses. Nodes SHOULD enforce bans but are not required to (sovereignty). In practice, most nodes will enforce because hosting banned content is a liability.

**False flag protection:**
- Flagging costs reputation (stake). If a governance vote rejects the flag, the flagger loses their stake.
- Severity:illegal false flags are penalized more heavily (-0.02 forfeited + additional -0.05 penalty for frivolous severe flags).
- Flag evidence is public and signed — fabrication is attributable and punishable.

### Shard Health

The protocol does not mandate shard replication monitoring in v0. Nodes that store shards SHOULD periodically verify that at least `data_shards + 1` copies of the full shard set remain available across known peers. When replication drops below threshold, nodes SHOULD re-distribute shards to willing peers.

### Content Discovery

Nodes maintain a local index of content they've synced (hosted, replicated, and proposed). Search is local in v0 — you can only discover content you've already received metadata for via gossip or sync.

Tags in SHARE and REPLICATE_REQUEST messages enable basic filtering. Nodes MAY implement full-text search over content descriptions locally.

A network-wide content discovery protocol (tag indices, semantic search, recommendation) can be proposed via the network once v0 is operational.

---

## 7. Proposals

### Message Types

#### REQUEST

A broadcast need. "I need something."

```json
{
  "type": "REQUEST",
  "payload": {
    "title": "<string>",
    "body": "<string — markdown>",
    "tags": ["<string>", ...],
    "expires": <unix_milliseconds | null>
  }
}
```

#### PROPOSE

A proposed solution. May reference a request, or stand alone.

```json
{
  "type": "PROPOSE",
  "payload": {
    "title": "<string>",
    "body": "<string — markdown description>",
    "content_hash": "<sha256_hex>",
    "content_type": "<mime_type>",
    "content_size": <bytes>,
    "content_url": "<optional — where to fetch the artifact>",
    "content_inline": "<optional — base64, max 1 MiB>",
    "request_id": "<message_id | null — if the referenced request has expired, this proposal still stands on its own>",
    "supersedes": "<message_id | null — previous proposal this replaces>",
    "claims": [
      {
        "statement": "<human-readable claim>",
        "verifiable": <boolean>,
        "evidence": "<optional — URL or description>"
      }
    ],
    "tier": "standard | constitutional",
    "eol": <unix_milliseconds | null>,
    "voting_deadline": <unix_milliseconds>
  }
}
```

**Content delivery:** Small artifacts (< 1 MiB) MAY be inlined as base64 in `content_inline`. The 1 MiB limit applies to the original artifact size before base64 encoding. Larger artifacts are referenced by `content_hash` and distributed using erasure-coded shards (see below). The hash ensures integrity regardless of source.

**Claims:** Verifiable assertions about the proposal. Signal for evaluation. The `claims` array MUST NOT exceed **50 entries**. Each claim's `statement` + `evidence` MUST NOT exceed **2 KiB** combined.

**EOL:** Only for protocol change proposals. When the previous protocol version dies.

**Voting deadline:** Required. Proposals MUST specify a deadline (suggested default: 14 days, max: 90 days). Votes received after the deadline MUST NOT be counted.

**Rate limiting:** Nodes are limited to **3 PROPOSE messages per 7-day rolling window**. This prevents spam flooding. Additionally, nodes MUST have reputation ≥ **0.3** to submit proposals — fresh and penalized nodes cannot propose until they've earned trust through contribution (see Capability Ramp in §9). Nodes MUST reject proposals from nodes below 0.3. Proposal rate limits transfer across KEY_ROTATE, like reputation and tenure. The new key inherits the old key's proposal count for the rolling window. For identities with authorized keys (§1 Identity Linking), the proposal rate limit applies to the identity as a whole — 3 proposals per 7-day window across all keys in the identity.

#### WITHDRAW

Author withdraws a proposal.

```json
{
  "type": "WITHDRAW",
  "payload": {
    "proposal_id": "<message_id>",
    "reason": "<optional string>"
  }
}
```

Only the original author (verified by `from` key) can withdraw. For proposals authored by an authorized key (§1 Identity Linking), any key in the same identity (including root) MAY withdraw. Nodes MUST stop counting new votes for withdrawn proposals. Nodes that have already locally adopted a proposal are not affected by a later withdrawal.

#### COMMENT

Discussion on proposals and requests:

```json
{
  "type": "COMMENT",
  "payload": {
    "target_id": "<message_id — proposal or request being discussed>",
    "body": "<string — markdown>",
    "parent_comment_id": "<message_id | null — for threading>"
  }
}
```

COMMENT messages are broadcast on `/valence/proposals`. They are lightweight, signed, and attributed. Comments do not affect governance — they are signal for evaluation, not votes. Rate limits scale with reputation: **1 per hour** at rep 0.3, **5 per hour** at rep 0.5, **10 per hour** at rep 0.8+. Additionally, comments are limited to **3 per identity per proposal per rolling 24-hour window** to prevent thread flooding.

Comments on withdrawn or archived proposals SHOULD still be accepted (post-mortems are valuable).

### Proposal Lifecycle

There is no global state machine. Each node tracks its own view:

- **Open** — proposal exists, votes accumulating, deadline not reached
- **Converging** — weighted endorsement exceeds local threshold
- **Ratified** — node considers the proposal accepted by the network
- **Rejected** — weighted rejection exceeds local threshold, or deadline passed without ratification
- **Adopted** — this specific node has applied the proposal locally and broadcast an ADOPT message
- **Expired** — voting deadline passed
- **Withdrawn** — author withdrew

### Adoption

When a node locally adopts a proposal, it broadcasts an `ADOPT` message:

```json
{
  "type": "ADOPT",
  "payload": {
    "proposal_id": "<message_id>",
    "success": <boolean>,
    "notes": "<optional string>"
  }
}
```

Adoption messages serve as usage attestations — they provide concrete signal that a proposal was applied and whether it worked. They feed into reputation (§9) and help other nodes evaluate proposals.

The `success` field allows nodes to report that they adopted something and it broke — this is valuable negative signal.

**Honest adoption is not provable in v0.** Any node can claim adoption without proof. This is a known limitation. The mitigation is statistical: fabricated adoption messages from colluding nodes will correlate with other collusion indicators (§11). A verifiable adoption mechanism (e.g., proof of execution) can be proposed via the protocol.

### Supersession

Edits to proposals are expressed as new proposals with `supersedes` set to the original proposal's ID. This is simpler than a separate edit mechanism — an edit is just a new proposal that references what it improves. The community votes on the new proposal independently.

Implementations SHOULD limit supersession chain display to 5 levels. Proposals SHOULD NOT supersede proposals deeper than 10 levels — nodes MAY reject proposals with `supersedes` referencing a chain deeper than 10.

### Content Delivery

Proposals reference content by `content_hash`. Content MUST be replicated (§6) before it can be proposed — voters must be able to access the artifact to evaluate it. Small artifacts (< 1 MiB) MAY be inlined as base64 in `content_inline` (the 1 MiB limit applies to the original artifact size before base64 encoding). Larger artifacts use the erasure-coded shard system described in §6.

Protocol change proposals MUST use 'standard' or 'resilient' erasure coding (§6). Nodes SHOULD reject protocol change proposals with 'minimal' coding.

`content_url` remains as a fallback for HTTP-hosted artifacts.

---

## 8. Votes

### VOTE Message

```json
{
  "type": "VOTE",
  "payload": {
    "proposal_id": "<message_id>",
    "stance": "endorse | reject | abstain",
    "reason": "<optional string>"
  }
}
```

### Vote Rules

- One vote per node per proposal. A second vote from the same `from` key supersedes the first. For identities with authorized keys (§1 Identity Linking), all keys share one vote per proposal. A vote from any key is attributed to the root identity. The standard supersession rule applies — a later vote from any key in the identity replaces the earlier one. If two votes from the same voter have the same `timestamp`, the vote with the lexicographically later `id` supersedes.
- Votes are weighted by the voter's reputation (see §9).
- Nodes MUST have reputation ≥ 0.3 to submit votes (see §9 Capability Ramp). Votes from nodes below this threshold MUST be rejected.
- **Abstain** votes count toward participation quorum but do NOT affect the endorse/reject ratio. This lets nodes signal "I showed up but don't have a strong opinion" without being forced to pick a side. Abstention is particularly important for constitutional proposals where participation quorum matters.
- Nodes MAY vote on their own proposals. The weight is the same.
- Votes received after the proposal's `voting_deadline` MUST NOT be counted.

### Local Evaluation

Each node evaluates proposals independently. Nodes MUST re-evaluate proposal status whenever new votes (including superseding votes) are received. There is no global "this passed" event.

A node considers a proposal ratified when **all**:

1. **Minimum voters:** At least **3** distinct nodes have voted (absolute minimum). For minimum voter counts, an identity with authorized keys (§1) counts as one voter regardless of how many keys voted.
2. **Quorum met:** Total reputation weight of all voters ≥ **quorum_weight** (see below)
3. **Threshold met:**
   ```
   weighted_endorse / (weighted_endorse + weighted_reject) >= local_threshold
   ```

Where:
- `weighted_endorse` = sum of `vote_time_reputation` for all endorse votes
- `weighted_reject` = sum of `vote_time_reputation` for all reject votes
- `vote_time_reputation` = the voter's reputation **at the time the vote was created** (the vote message's `timestamp` field), not when it was received. Nodes reconstruct this from the most recent REPUTATION_GOSSIP they hold with an `assessment_timestamp` prior to the vote's timestamp. This reduces vote weight divergence across nodes compared to receive-time snapshots, though nodes with different REPUTATION_GOSSIP histories will still reconstruct slightly different weights. This divergence is bounded and acknowledged in §15.
- `local_threshold` = configured per-node (suggested default: **0.67**)
- Nodes SHOULD use a higher threshold for protocol change proposals (suggested: **0.80**)

**Split ratification:** Because reputation data is gossip-informed and not globally consistent, nodes MAY reach different ratification conclusions for the same proposal. When a node observes ADOPT messages for a proposal it considers not-ratified, it SHOULD re-evaluate using any new reputation data available. If still not ratified locally, the node is not obligated to adopt — local evaluation is authoritative. Persistent disagreement is expected to be rare and self-correcting as reputation data converges.

**Close-margin confirmation:** When a proposal's `voting_deadline` passes and its weighted endorsement ratio falls within **±0.02** of the applicable threshold (e.g., 0.65–0.69 for standard proposals at 0.67), nodes MUST enter a **7-day confirmation period** before making a final ratification determination. During this period, nodes SHOULD actively request REPUTATION_GOSSIP for all voters in the proposal via the sync protocol, integrating any new assessments to narrow the reputation reconstruction divergence. After the confirmation period, nodes make their final local determination. This does not guarantee convergence but significantly reduces the probability of split ratification on close votes.

### Network Phases

The network has two governance thresholds. Both require sustained node count AND can be accelerated by genuine activity.

**Definitions:**
- `active_nodes` = nodes with a PEER_ANNOUNCE received within the last 30 minutes (consistent with peer expiry in §4)
- `total_known_reputation` = sum of locally-computed reputation for all active nodes
- "Sustained" means the node has observed ≥ N active peers (per PEER_ANNOUNCE received within the last 30 minutes) for every consecutive clock-hour in the sustain period. A single clock-hour below threshold resets the sustain clock.

**< 16 nodes:** The protocol is frozen. Content proposals (skills, configs, documents, code) work — that's what the network is for. But **all governance proposals are rejected**. Share, collaborate, build reputation. No rule changes.

**≥ 16 nodes, sustained:** Standard (Tier 1) governance activates. The sustain period depends on network activity:

```
sustain_days = max(3, 7 / activity_multiplier)
```

Where `activity_multiplier` (range 1.0–2.33) is:

```
activity_multiplier = 1.0 + min(1.33, adopted_proposals × 0.33 + earned_rep_delta × 2.0 + challenge_pairs × 0.1)
```

- `adopted_proposals` = number of content proposals with ≥ 3 distinct ADOPT messages from distinct nodes, during the sustain evaluation window
- `earned_rep_delta` = sum of (current_rep - initial_rep) across all active nodes where current_rep > initial_rep, capped at 1.0 for this formula
- `challenge_pairs` = number of distinct (challenger, challenged) node pairs that have completed at least one STORAGE_CHALLENGE/STORAGE_PROOF exchange, capped at 10 for this formula

All inputs are derived from publicly observable protocol messages. The formula is evaluated locally by each node using its own view of the network.

At minimum activity (multiplier 1.0): 7 days. At high activity (multiplier 2.33): 3 days. The floor of 3 days is non-negotiable — gossip convergence and anomaly detection need time.

Activity acceleration does NOT apply to the constitutional threshold.

**≥ 1,024 nodes for 30 consecutive days:** Constitutional (Tier 2) governance activates. No activity acceleration — constitutional governance should be deliberately slow to unlock. The 30-day sustain requirement prevents Sybil-driven threshold gaming.

### Cold Start

For the first **30 days** after standard governance activates, voting uses **headcount mode**: majority of known nodes must vote, >67% endorse. No reputation weighting. During cold start headcount voting, identities with authorized keys count as one node. This prevents a tiny number of early grinders from dominating governance before reputation has differentiated.

Proposals are evaluated under the voting mode in effect when their `voting_deadline` expires. A proposal submitted during cold start whose deadline falls after cold start ends is evaluated under weighted voting.

However, proposals submitted during cold start MUST additionally require that at least 50% of active nodes have cast a vote (endorse, reject, or abstain), even if evaluated under weighted voting. This prevents gaming the quorum discontinuity at the cold start boundary. The 50% headcount requirement expires **60 days after cold start ends** — proposals submitted during cold start whose voting deadline falls beyond this grace period are evaluated under normal weighted voting rules only. This prevents the headcount floor from becoming permanently onerous for early proposals in a growing network.

After cold start ends, weighted voting activates:

- Standard quorum: `max(active_nodes × 0.1, total_known_reputation * 0.10)` (scales with network, floor tied to network size)
- Constitutional quorum: `total_known_reputation * 0.30` (30% of the network's reputation must participate)

### Proposal Tiers

Once critical mass is reached, proposals fall into two tiers:

#### Tier 1: Standard

Skills, configs, documents, code, feature proposals. The normal business of the network.

- **Threshold:** 0.67 weighted endorsement
- **Quorum:** standard quorum weight
- **Voting deadline:** 14 days default, 90 days max

#### Tier 2: Constitutional

Changes to the protocol's core safety properties. A proposal is constitutional if it modifies any of:

- The reputation floor
- Sybil resistance mechanisms (VDF or equivalent)
- The voting threshold or quorum formula
- The constitutional tier itself (meta-governance)
- Key rotation / identity mechanisms
- The critical mass threshold

Constitutional proposals require:

- **Threshold:** 0.90 weighted endorsement
- **Quorum:** 30% of total known network reputation (not a multiplier — a percentage of the whole network)
- **Voting deadline:** 90 days minimum
- **Cooling period:** 30 days between ratification and activation (allows nodes to evaluate and leave if they disagree). The cooling period begins at the proposal's `voting_deadline` (deterministic, all nodes agree). Nodes MUST NOT broadcast ADOPT messages for constitutional proposals until `current_time >= voting_deadline + 30 days`. ADOPT messages received before this time MUST be rejected.

The constitutional tier does NOT make anything immutable — the network can still change these properties. It just requires overwhelming consensus, a long deliberation period, and gives dissenting nodes time to exit.

Below 16 nodes, governance is disabled. At 16, standard governance activates. At 1,024, constitutional governance unlocks and all v0 protocol parameters become Tier 2 protected.

---

## 9. Reputation

### Score

For identities with authorized keys (§1), there is one reputation score per identity. All actions by any authorized key affect the root identity's reputation.

Each node maintains a reputation score for every peer it knows about.

| Field | Type | Description |
|---|---|---|
| `overall` | float | 0.0–1.0. **Hard cap.** Reputation cannot exceed 1.0. |
| `by_domain` | map | Domain-specific scores (e.g., `{"skills": 0.7, "config": 0.4}`) |
| `verification_count` | integer | How many times this peer has verified claims |
| `stake_at_risk` | float | Reputation currently at risk from disputed claims |

### Initial Score

New nodes start at **0.2**. This low starting point means:

- **Below 0.2** = penalized. Clear signal of bad behavior.
- **At 0.2** = new, unproven. Default state.
- **0.3–0.5** = contributing, building trust.
- **0.5+** = established contributor.
- **0.8+** = highly trusted veteran.

The low start ensures that abandoning a penalized identity and re-registering is *always* a loss (any rep above 0.2 is forfeited). It also means earned reputation is clearly distinguishable from default reputation.

### Floor

Reputation cannot drop below **0.1**. This prevents permanent exclusion and allows recovery.

### Earning Reputation

Reputation enters the system through two channels: **creation** (new reputation from genuinely valuable actions) and **transfer** (reputation moved between identities for services rendered).

#### Created (new reputation entering the system)

| Action | Reward | Notes |
|---|---|---|
| Proposal adopted by peers | +0.005 per ADOPT message received | Capped at +0.05 per proposal. Split 70/30 with original content host if `origin_share_id` is set (§6 Provenance). |
| Claim verified true | +0.001 | Confirmation reward |
| Claim found false (by you) | +0.005 | Contradiction reward |
| First to find contradiction | +0.01 | First-finder bonus |
| Uptime (continuous availability) | +0.001 per day | Caps at 30 days accumulation. The baseline reward for being a reliable network participant. |

These are the ONLY sources of new reputation in the system. They reward: creating useful content, honest verification, and reliable availability.

#### Transferred (reputation moved between identities)

| Action | From | To | Rate | Notes |
|---|---|---|---|---|
| Storage rent | Content uploader | Storage providers | Monthly, market-priced (§6) | Providers must pass challenges to receive payment |
| Replication of proposed content | Proposal author (from adoption rewards) | Storage providers | 30% of per-proposal adoption cap | Carved from the author's +0.05 adoption pool, NOT additional creation |

Storage is a service market — uploaders pay, providers earn. No new reputation is created.

### Losing Reputation

#### Destroyed (reputation leaving the system)

| Action | Penalty | Notes |
|---|---|---|
| Proposal with false claims | -0.003 per false claim | Contradiction penalty |
| Failed storage challenge | -0.01 | Claimed to store shard but failed adjacency challenge. Also forfeits rent for that billing cycle. |
| Collusion detected | -0.05 | See §11 |
| Inactivity | -0.02 per month | After 30 days inactive. A node is "active" if it has authored and broadcast at least one signed protocol message (VOTE, PROPOSE, ADOPT, STORAGE_PROOF, or PEER_ANNOUNCE) within the window. PEER_ANNOUNCE alone suffices — it proves liveness without requiring governance participation. |
| Content flagged and upheld | -0.05 | Source identity penalized when flag is upheld by governance |
| Severe content ban | all reputation forfeited | DID permanently banned (§6 Content Flagging) |
| False flag (rejected) | -0.005 to -0.07 | Flagger loses stake + additional penalty for frivolous severe flags |

### Reputation Flow Summary

```
                    ┌─────────────┐
   Creation ──────► │  Total Rep  │ ◄────── Transfer (zero-sum)
  (adoption,        │  in System  │         (storage rent:
   verification,    └──────┬──────┘          uploader → provider)
   uptime)                 │
                    Destruction
                  (penalties, decay,
                   inactivity, bans)
```

Total system reputation grows slowly via creation (adoption + verification + uptime) and shrinks via destruction (penalties + decay). Storage is a zero-sum transfer market that moves reputation between participants without changing the total. This keeps reputation meaningful as the network scales.

### Capability Ramp

Fresh identities have limited capabilities. Reputation gates access to higher-impact actions:

| Reputation | Capabilities Unlocked |
|---|---|
| 0.2 (starting) | Sync, browse, adopt, **store shards and earn rent** (the primary on-ramp), uptime accrual |
| 0.3 | Propose content (small, storage limit applies), replicate content, vote (low weight), flag (dispute only) |
| 0.5 | Full proposal size limits, all flag severities, full vote weight |
| 0.8+ | Maximum storage allocation, high vote weight |

The capability ramp is the primary defense against abuse. Fresh DIDs can contribute (store shards, earn rent, build reputation) but cannot impact the network (no proposals, no votes, no flags) until they've proven themselves through work. A banned actor regenerating a new DID starts at 0.2 with minimal capabilities — the 30-second VDF and the reputation ramp make re-entry structurally costly.

### Velocity Limits

| Limit | Value | Applies |
|---|---|---|
| Maximum daily gain | 0.02 | Above 0.2 (starting score) |
| Maximum weekly gain | 0.08 | Above 0.2 |

**Recovery below starting score is uncapped.** A node penalized below 0.2 can recover at whatever rate their contributions earn, with no daily/weekly cap, until they reach 0.2 again. Above 0.2, velocity limits apply normally.

This completely eliminates identity recycling: re-registering always gives you 0.2, which is the *minimum* any non-penalized node has. There is never an advantage to abandoning an identity.

### Reputation Propagation

Reputation is **locally computed** and **gossip-informed**.

When Node A evaluates Node B's reputation:

1. Start with A's direct observations of B
2. Query peers for their signed assessments of B
3. Weight each peer's assessment by A's trust in that peer
4. Combine:

```
reputation(B) = α × direct_observations(B) + (1-α) × Σ(trust(P) × assessment_P(B)) / Σ(trust(P))
```

Where:
- `α` = **scales with observation count**: `min(0.6, observation_count / 10)`. With 0 direct observations, α = 0 (pure peer assessment). At 6+ observations, α = 0.6. An observation is any protocol message authored by the subject identity that this node has directly received and verified: PROPOSE, VOTE, ADOPT, STORAGE_PROOF, PEER_ANNOUNCE, or REPUTATION_GOSSIP. For identities with authorized keys (§1), observations are deduplicated per identity per message type per hour — if an identity sends 10 PEER_ANNOUNCE messages from 10 keys in one hour, it counts as 1 observation, not 10. Additionally, deduplicate by message `id` — each unique message counts as at most one observation. This prevents multi-key identities from reaching α=0.6 faster than single-node identities and avoids a "stranger danger" dynamic where unknown nodes are systematically penalized. α growth is bounded by the per-identity-per-type-per-hour dedup rule (§1). Additionally, observation count grows at most 6 per hour per subject identity (one per message type). New nodes cannot immediately max α — reaching α=0.6 requires at least 1 hour of diverse activity from the subject.
- When `α = 0` (no direct observations), `direct_observations(B)` is not used. The node's reputation is entirely peer-informed until direct interaction occurs. When α = 0, the peer-informed reputation MUST be capped at the starting reputation (0.2) until assessments from at least 3 independent assessors from distinct ASNs have been received. This prevents reputation injection via sock puppet attestations.
- Sum is over peers P that have shared signed assessments of B
- `trust(P)` = A's direct reputation score for P (NOT recursive — to avoid circular dependency)

**Evaluation order (fixed-point):** All intermediate values use 64-bit scaled integers (×10,000). The formula evaluates as:
```
peer_sum = Σ(trust_i × assessment_i)     // each already ×10,000, product is ×10^8
peer_weight = Σ(trust_i)                  // ×10,000
peer_avg = peer_sum / peer_weight         // back to ×10,000 scale
result = (α × direct + (10000 - α) × peer_avg) / 10000
```
Truncation occurs only on `result` for storage/transmission.

Trust is **one level deep**: you trust your peers based on your direct experience with them, and their assessments influence your view of nodes you haven't directly observed.

### Signed Reputation Attestations

Reputation gossip MUST be signed. Each assessment is attributable.

```json
{
  "type": "REPUTATION_GOSSIP",
  "payload": {
    "assessments": [
      {
        "node_id": "<hex_public_key>",
        "overall": <float>,
        "by_domain": { ... },
        "observation_count": <integer>,
        "last_observed": <unix_milliseconds>,
        "assessment_timestamp": <unix_milliseconds>
      }
    ]
  }
}
```

The `assessment_timestamp` field records when the assessor last updated this specific assessment. Nodes MUST reject assessments with an `assessment_timestamp` older than the most recent assessment they hold for the same (assessor, subject) pair. This creates a monotonic sequence per assessment relationship. After KEY_ROTATE, the new key's assessments for a given subject MUST have `assessment_timestamp` ≥ the most recent assessment from the old key for the same subject. The monotonicity chain spans key rotation.

Because assessments are signed by the sender, they are **attributable and comparable**. If Node A's assessments of C vary significantly when compared by B and D, this is noted but **not automatically penalized** — reputation is locally computed and legitimately changes over time. Instead, signed attestations serve as an audit trail: patterns of extreme or strategic variation can inform collusion detection (§11) but honest temporal variation is expected.

Nodes SHOULD share reputation gossip every **15 minutes**, covering **10 random peers** per message. Each REPUTATION_GOSSIP message includes assessments for up to 10 peers. Nodes with more than 10 assessments SHOULD rotate which peers are included in each 15-minute interval to ensure full coverage over time.

### Trust Model

The protocol has no dedicated trust subsystem. Trust is **derived from existing protocol signals**, not stored or declared.

**Node trust** is reputation — earned through proposals, voting, storage, and verification (this section). It's the only trust score the protocol defines.

**Content trust** is not defined by the protocol. The signals exist — vote margins, voter reputation at vote time, adoption count and success rate, supersession depth, source diversity (ASN distribution of endorsing nodes) — but how a node combines them is a local decision. A node optimizing for safety might weight voter diversity heavily. A node optimizing for speed might trust high-rep endorsements alone. Both are valid.

The protocol MUST make these signals available via gossip and sync (vote records are public, reputation attestations are signed, adoption messages are broadcast). The protocol MUST NOT prescribe a formula for content confidence. Agent autonomy means nodes evaluate content by their own criteria.

This is deliberate: a prescribed trust formula becomes a gaming target. When trust is locally computed from public signals, there is no single function to exploit — an attacker would need to fool each node's independent evaluation.

---

## 10. Sybil Resistance

### Verifiable Delay Function (VDF)

New nodes MUST compute a VDF proof before participating. This makes mass identity creation expensive.

**Parameters:**
- Algorithm: Iterated SHA-256 (v0). Acknowledged limitation: verification requires recomputation. A proper VDF (Wesolowski/Pietrzak) with fast verification can be proposed via the protocol.
- Difficulty: **1,000,000 iterations** (~30 seconds on commodity hardware)
- Input: The node's public key bytes
- Output: VDF proof (hash chain output + intermediate checkpoints)

**Iteration semantics:**
```
h[0] = SHA-256(public_key_bytes)
h[i] = SHA-256(h[i-1])           for i = 1..difficulty
output = h[difficulty]
checkpoint[k] = h[k × 100,000]   for k = 1..10
```

Reference implementations MUST include a test vector suite with known-good checkpoints for the standard difficulty.

**Intermediate checkpoints:** The proof includes hashes at every 100,000th iteration (10 checkpoints). Verifiers MUST verify at least **5 randomly selected segments** (recompute from checkpoint N to checkpoint N+1 and compare). Segment selection MUST use a CSPRNG and MUST NOT be derived from the VDF proof being verified (to prevent targeted forgery). This takes ~15 seconds but provides 99.97% detection rate against single-segment forgery (probability of evading = (5/10)^5 ≈ 3%). Full chain verification (all 10 segments, ~30 seconds) SHOULD be performed for the first peer accepted from each new ASN.

### Registration Flow

1. Generate Ed25519 keypair
2. Compute VDF proof over public key bytes
3. Include proof in auth handshake (§3)
4. Peers verify the proof (spot-check) before accepting the node

### VDF Proof Schema

```json
{
  "output": "<hex>",
  "input_data": "<hex — public key bytes>",
  "difficulty": 1000000,
  "computed_at": <unix_milliseconds>,
  "checkpoints": [
    {"iteration": 100000, "hash": "<hex>"},
    {"iteration": 200000, "hash": "<hex>"},
    ...
  ]
}
```

The `computed_at` field records when the VDF computation began. Nodes MUST reject proofs where `computed_at` is older than 24 hours or more than 5 minutes in the future (consistent with §2 timestamp tolerance).

### Rate Limiting

- Maximum 50 VDF verifications per day
- Maximum 5 new peers accepted per hour
- VDF proofs with `input_data` that doesn't match the sender's public key MUST be rejected
- VDF proofs MUST be fresh — nodes MUST reject VDF proofs where the earliest checkpoint timestamp (included in the proof, see schema below) is older than **24 hours**. VDF `input_data` remains `public_key_bytes` (not connection-specific), so a proof can be reused across multiple peer connections within its 24-hour validity window. Nodes MUST recompute their VDF proof at least once per 24 hours to maintain connectivity. This prevents indefinite proof reuse while keeping the cost of joining the network reasonable (one 30-second computation per day, not per connection)

---

## 11. Anti-Gaming

### Collusion Detection

Nodes SHOULD analyze voting patterns for collusion indicators:

1. **Voting correlation:** Flag groups of 3+ nodes with >95% vote correlation over 20+ proposals
2. **Registration timing:** Flag 3+ nodes whose VDF proofs were computed within a 24-hour window
3. **ASN clustering:** Flag when >25% of active voters share an ASN

Keys within the same identity (§1 Identity Linking) are exempt from collusion detection, including both vote correlation and VDF registration timing.

Detection is local — each node runs its own analysis. When collusion indicators are found:

- **WARNING severity:** Log, reduce trust in flagged nodes
- **HIGH severity:** Reduce reputation of flagged nodes by 0.05
- **Nodes MAY share collusion alerts** via proposals ("I've detected collusion, here's the evidence, should we act?")

### Tenure Limits

A **voting cycle** is a rolling 30-day window. A node is "active in a cycle" if it has **voted on or had proposals ratified** during that window. This tracks participation broadly, not just proposal authorship — a node that votes on everything but never proposes is still accumulating influence.

Nodes active in 6+ consecutive cycles receive a tenure penalty:

- **5% reduction** in vote weight per cycle after the 6th
- This prevents entrenchment — early participants can't permanently dominate
- Tenure tracks the **identity's history**, not the key. Key rotation (§1) transfers tenure along with reputation — rotating keys does NOT reset tenure
- Skipping cycles **decays** the tenure counter non-linearly: the first skipped cycle decays tenure by 1, the second consecutive skip by 2, the third by 3, and so on. To go from tenure 6 to tenure 0 requires 3 consecutive skips (1+2+3=6), costing 90 days of inactivity and −0.06 reputation from inactivity decay. The accelerating decay makes strategic oscillation expensive — a node at tenure 6 attempting to reset by skipping must endure increasing penalties per cycle, and recovery from the reputation loss takes additional time

### Diversity Scoring

When evaluating proposals, nodes MAY weight votes higher when they come from a diverse set of voters (measured by ASN diversity, federation membership, registration age).

---

## 12. Partition Detection

### Merkle Consistency

Nodes periodically compute a Merkle root over their active proposal set.

**Tree construction:**
- **Leaves:** SHA-256 hash of each active (non-withdrawn, non-expired) proposal's `id`
- **Ordering:** Leaves sorted lexicographically by `id`
- **Structure:** Binary Merkle tree, left-biased for odd leaf counts (last leaf promoted)
- **Hash function:** SHA-256 for internal nodes: `SHA256(left_child || right_child)`

When the active proposal set is empty, the Merkle root is the SHA-256 hash of the empty byte string (`e3b0c44298fc1c149afbf4c8996fb924...`).

During sync, nodes exchange Merkle roots in `SYNC_RESPONSE`. Divergent roots trigger a partition event.

### Severity Classification

**Proposal Merkle divergence:**

| Condition | Severity |
|---|---|
| < 5% proposal set difference | info |
| 5–20% difference | warning |
| > 20% difference | critical |

**Identity Merkle divergence** (see §5 Identity Merkle Tree):

| Condition | Severity |
|---|---|
| Divergence involves only `DID_LINK` or `KEY_ROTATE` (additive) | info |
| Divergence involves any `DID_REVOKE` (subtractive) | critical |
| Mixed additive + subtractive | critical |

Identity divergence is classified independently of proposal divergence. A node with matching proposal roots but divergent identity roots (involving DID_REVOKE) MUST treat this as critical and immediately fetch missing revocations.

### Proposal Retention

The active proposal set MUST be bounded to prevent unbounded growth. Retention policy:

- **Expired proposals** (past `voting_deadline` without ratification): archived after **7 days**
- **Withdrawn proposals:** archived after **7 days**
- **Rejected proposals** (weighted rejection exceeded threshold): archived after **30 days**
- **Ratified proposals:** archived after **180 days** (6 months)
- **Protocol change proposals:** never auto-archived (permanent record)

Archived proposals are removed from the active set and the Merkle tree. Nodes MAY retain archived proposals locally for historical reference but MUST NOT include them in Merkle root computation or sync responses (unless specifically requested).

**Archival is deterministic, not scheduled.** A proposal is archived the instant it meets the retention criteria above. When computing the Merkle root or responding to sync, implementations MUST exclude any proposal whose archival condition is met at the current time. This ensures all nodes agree on the active set at any given moment without coordinating archival schedules.

### Partition Merge

When partitions heal and nodes discover proposals they missed:

1. Sync all missing proposals and votes from both sides
2. Re-evaluate all proposals with the complete vote set (merged votes may change ratification status)
3. For **content proposals** (skills, configs, docs): contradictory ratifications can coexist. Nodes choose locally.
4. For **protocol change proposals**: contradictory ratifications are resolved by **timestamp priority** — the proposal with the earlier `voting_deadline` takes precedence. If deadlines are equal, the proposal with the lower `from` key (lexicographic) wins — public keys are not grindable (unlike `id`, which is derived from grindable timestamp inputs). Exception: if one proposal's `supersedes` field references the other, the superseding proposal takes precedence regardless of timestamp. The losing proposal is re-opened for voting under the merged network's state.

**Content and storage state:** During a partition, content may have divergent lifecycle states (active on one side, grace period or decayed on the other). On merge: content that is active on *either* partition survives (union semantics). Rent obligations resume from the merge point — no back-rent is owed for the partition period. Flag state merges via union: flags from both partitions accumulate. If the deletion threshold was met on either side, the merged network respects it. Content provenance (`origin_share_id`) uses the earliest SHARE timestamp across both partitions.

**Identity state:** DID_LINK and DID_REVOKE messages merge via union — identity changes from either partition propagate to the merged network. When conflicting states exist (a key linked on one side, revoked on the other), DID_REVOKE takes precedence (revocation is irreversible). Votes cast by a key that was revoked (per the DID_REVOKE timestamp) before the vote's timestamp are invalidated on merge. The 60-day voting cooldown for re-registered revoked keys starts from the DID_REVOKE timestamp, not the merge point.

This is a deterministic tie-breaking rule, not a justice system. It ensures all nodes converge to the same protocol state after a partition heals, which is necessary for the network to function. Content proposals don't need this because they don't affect the protocol itself.

---

## 13. Protocol Evolution

### Self-Hosting

v0 governs its own evolution. Protocol changes are **Tier 2 (constitutional) proposals** once the network reaches critical mass. They require:
- `eol` field set (when the previous version dies)
- 0.90 threshold, 30% network quorum, 90-day minimum voting, 30-day cooling period
- Pre-critical-mass: protocol is frozen, no changes possible

### Version Negotiation

Every message carries `"version": 0`. When v1 is ratified, nodes that adopt it start sending `"version": 1`. During the grace period:

1. Nodes MUST accept messages with version N or N+1
2. Nodes MUST respond in the version the peer sent
3. Nodes MUST cleanly reject (not silently corrupt) messages from unrecognized versions

After EOL, nodes on version N stop accepting messages from version N-1 peers.

### Upgrade Path

A protocol change proposal MUST include:
- The complete new specification (or a diff against the current spec)
- The EOL date for the current version
- A migration path

### EOL Timelines

Set by the community as part of the proposal:
- **Minor changes:** 30 days minimum
- **Major changes:** 90+ days minimum

### Constraints

- Changes MUST NOT cause silent misinterpretation between versions during grace period
- Version negotiation MUST cleanly fail rather than corrupt
- The network can evolve into anything it decides — if it decides to suck, that sucks

---

## 14. Error Handling

### Malformed Messages

Messages that fail parsing, signature verification, or timestamp validation MUST be silently dropped. Nodes MUST NOT propagate invalid messages.

### Unknown Message Types

Messages with unrecognized `type` values SHOULD be propagated but not processed, enabling forward compatibility. However, unknown types are subject to **per-sender rate limiting**: nodes MUST NOT propagate more than **10 unknown-type messages per sender per hour**. Messages exceeding this limit are silently dropped. This prevents DoS via garbage message flooding while preserving forward compatibility for legitimate new message types.

### Unknown Fields

Messages with extra fields beyond the schema MUST be accepted (ignore unknown fields). Implementations MUST NOT reject messages with additional fields. This enables extension without protocol version bumps.

### Missing Proposals

When a node receives a VOTE for a proposal it hasn't seen:

1. Store the vote
2. Request the missing proposal from the vote's sender via sync
3. Apply the vote once the proposal arrives

Votes are valid even if they arrive before their proposal (due to gossip ordering).

### Unresponsive Peers

Nodes SHOULD implement exponential backoff for unresponsive peers, with a maximum retry interval of 10 minutes. After 30 minutes without response, the peer SHOULD be pruned from the active peer table (but not forgotten — it can re-announce).

---

## 15. Open Questions

These are acknowledged gaps in v0. They may be addressed by protocol proposals, by operational experience, or by future spec revisions before v0 is finalized.

### Reputation Merkle Tree

v0 has Merkle trees for identity state (§5) and proposal state (§12) but not for reputation state. Reputation divergence detection relies on the coarse `reputation_summary_hash` in snapshots and REPUTATION_CURRENT cross-peer comparison. This is sufficient for bootstrap networks but won't scale efficiently to 10K+ nodes where Merkle-based narrowing would reduce sync bandwidth. A reputation Merkle tree can be proposed via the protocol when operational experience demonstrates the need.

### Large-Scale Reputation Gossip

At 10,000 nodes, reputation gossip every 15 minutes at 10 assessments each produces ~111 messages/second. Mitigations to consider:
- Adaptive gossip frequency (less frequent as network grows)
- Only gossip about nodes whose reputation changed
- Hierarchical gossip (cluster-level summaries)

This is a scaling problem that doesn't affect bootstrap. Can be addressed by protocol proposal when the network reaches a size where it matters.

### VDF Hardware Heterogeneity

A 30-second VDF on commodity hardware is 3 seconds on a fast server. The difficulty parameter doesn't equalize across hardware. This means well-resourced attackers face a lower cost per identity. Accepted trade-off for v0: it raises the floor, not the ceiling. A proper VDF (Wesolowski/Pietrzak) with verifiable timing can be proposed via the protocol.

### Blacklisting / Banning

v0 has no mechanism to permanently exclude a node. A node with reputation at floor (0.1) can still participate, just with minimal weight. This is intentional — permanent exclusion is antithetical to the protocol's values. Nodes can locally block specific peers at the implementation level. The network can propose a formal ban mechanism if it decides it needs one.

### Constitutional Participation Incentives

The constitutional quorum (30% of network reputation) combined with explicit abstention (§8) ensures meaningful participation. But turnout may still be low without incentives.

Open design questions:
- **Voting incentives:** Should there be a small reputation reward for voting (including abstaining) on constitutional proposals, to drive turnout? Risk: incentivizing uninformed votes. But without it, participation may rely on civic duty alone.
- **Minimum node-count participation:** In addition to the reputation-weighted quorum, should constitutional proposals require a minimum percentage of *nodes* (not just reputation weight) to vote? This prevents a small number of high-rep nodes from meeting quorum alone.

These need to be resolved before the constitutional tier activates (1,024 nodes), not before v0 ships.

### Hierarchical Delegation

Identity authorized key sets (§1) are deliberately flat — one root, N children. Hierarchical delegation (root → sub-root → child) introduces complexity: transitive revocation, cascading trust, ambiguous voting authority. It can be proposed via the protocol once there's operational experience with flat authorized key sets.

### Acknowledged Limitations (By Design)

These are inherent properties of the architecture. They are not bugs — they are trade-offs that come with a subjective, gossip-based consensus system:

- **Vote weight divergence:** The same vote has slightly different weight on different nodes due to gossip latency affecting reputation snapshots. This is inherent to combining local reputation with arrival-time snapshots.
- **Information asymmetry:** Well-connected nodes get fresher reputation data, leading to better decisions and a Matthew effect. Mitigated by anti-fragmentation (§4) but not eliminated.
- **Collusion detection evasion:** Sophisticated colluders can add noise to stay below detection thresholds. The detection catches unsophisticated collusion and raises the cost of coordination.
- **Selective gossip:** A well-connected node can selectively relay votes, subtly influencing which proposals get seen. Anti-fragmentation and diverse connections mitigate but don't eliminate this. Vote receipts ("I've seen vote X" attestations) could be proposed as a network service to make selective relay detectable.
- **Bootstrap founder advantage:** Early nodes accumulate reputation and influence first. The adaptive quorum (§8) and tenure penalties (§11) limit this but don't eliminate it. This is the cost of a permissionless bootstrap.

---

## Constants Summary

| Constant | Value | Context |
|---|---|---|
| Ed25519 key size | 32 bytes (private), 32 bytes (public), 64 bytes (signature) | §1 |
| Key rotation grace period | 1 hour (from KEY_ROTATE.timestamp) | §1 |
| Canonicalization | RFC 8785 (JCS) | §2 |
| Max message payload | 8 MiB | §2 |
| Timestamp tolerance | ±5 minutes | §2 |
| Inline content limit | 1 MiB (pre-base64) | §7 |
| GossipSub topics | 3 | §3 |
| GossipSub parameters | D=6, D_low=4, D_high=12, fanout=6, heartbeat=1s, message_cache_ttl=3 heartbeats | §3 |
| Peer announce interval | 5 minutes | §4 |
| Peer expiry | 30 minutes | §4 |
| Anti-fragmentation interval | 10 minutes | §4 |
| Max ASN fraction | 25% | §4 |
| Min distinct ASNs | 4 | §4 |
| Dedup cache size | 100,000 entries (LRU) | §5 |
| Message max age | 24 hours | §5 |
| Voting deadline default | 14 days | §7 |
| Voting deadline max | 90 days | §7 |
| Supersession chain display limit | 5 levels | §7 |
| Supersession chain depth limit | 10 levels | §7 |
| SHARD_QUERY rate limit | 10 per sender per minute | §6 |
| Minimum voters | 3 | §8 |
| Standard governance threshold | 16 nodes sustained 3-7 days (activity-dependent) | §8 |
| Constitutional governance threshold | 1,024 nodes sustained 30 days | §8 |
| < 16 nodes | frozen (content only) | §8 |
| Cold start period | 30 days after governance activation (headcount voting, no early exit) | §8 |
| Standard quorum | max(active_nodes × 0.1, total_rep × 0.10) | §8 |
| Standard threshold | 0.67 | §8 |
| Constitutional threshold | 0.90 | §8 |
| Constitutional quorum | 30% of total network reputation | §8 |
| Constitutional voting minimum | 90 days | §8 |
| Constitutional cooling period | 30 days (from voting_deadline) | §8 |
| Default vote threshold | 0.67 | §8 |
| Proposal tiers | standard (0.67), constitutional (0.90) | §8 |
| Initial reputation | 0.2 | §9 |
| Reputation floor | 0.1 | §9 |
| Max daily reputation gain | 0.02 | §9 |
| Max weekly reputation gain | 0.08 | §9 |
| Monthly inactivity decay | 0.02 | §9 |
| Direct observation weight (α) | min(0.6, observation_count / 10) | §9 |
| Reputation gossip interval | 15 minutes | §9 |
| Reputation gossip batch | 10 random peers | §9 |
| VDF verification segments | 5 minimum (of 10) | §10 |
| VDF difficulty | 1,000,000 iterations | §10 |
| VDF checkpoints | every 100,000 iterations | §10 |
| VDF max verifications/day | 50 | §10 |
| Max new peers/hour | 5 | §10 |
| Vote correlation threshold | 95% over 20+ proposals | §11 |
| Reputation hard cap | 1.0 | §9 |
| Recovery below 0.2 | uncapped (no velocity limit) | §9 |
| Proposal rate limit | 3 per node per 7 days | §7 |
| Min rep to propose | 0.3 | §7 |
| Vote stances | endorse, reject, abstain | §8 |
| Tenure penalty onset | 6 cycles | §11 |
| Tenure penalty factor | 0.95× per cycle | §11 |
| Tenure decay | accelerating: −1, −2, −3... per consecutive skip | §11 |
| Collusion penalty | -0.05 | §11 |
| Adoption reward | +0.005 per ADOPT (max +0.05 per proposal) | §9 |
| Claim verified true reward | +0.001 | §9 |
| Claim found false reward | +0.005 | §9 |
| First contradiction bonus | +0.01 | §9 |
| Uptime reward | +0.001 per day (caps at 30 days accumulation) | §9 |
| False claim penalty | -0.003 per false claim | §9 |
| Failed storage challenge penalty | -0.01 | §9 |
| Content flagged upheld penalty | -0.05 | §9 |
| Gain dampening exponent | 0.75 (for multi-key identities) | §1 |
| Numeric precision | ×10,000 fixed-point integers | §2 |
| Unknown-type rate limit | 10 per sender per hour | §14 |
| Voting cycle | 30-day rolling window | §11 |
| Minor change EOL minimum | 30 days | §13 |
| Major change EOL minimum | 90 days | §13 |
| Expired proposal archive | 7 days | §12 |
| Withdrawn proposal archive | 7 days | §12 |
| Rejected proposal archive | 30 days | §12 |
| Ratified proposal archive | 180 days | §12 |
| Partition merge rule | timestamp priority, then `from` key tie-break | §12 |
| Default erasure coding | standard (5-of-8) | §6 |
| Peer backoff maximum | 10 minutes | §14 |
| Storage challenge nonce | 32 bytes random hex | §6 |
| Assessment timestamp monotonicity | per (assessor, subject) pair | §9 |
| Peer-informed rep cap (α=0) | 0.2 until 3 assessors from distinct ASNs | §9 |
| Protocol proposal min erasure coding | standard or resilient | §6 |
| Empty Merkle root | SHA-256 of empty byte string | §12 |
| Sustained threshold | ≥ N peers every clock-hour in sustain period | §8 |
| Cold start headcount floor | 50% of active nodes for cold-start-submitted proposals | §8 |
| Activity multiplier cap | 2.33 (sustain floor 3 days) | §8 |
| VDF iteration | h[0]=SHA-256(pubkey), h[i]=SHA-256(h[i-1]) | §10 |
| Authorized keys per identity | no limit (one vote per identity per proposal) | §1 |
| Reputation gain dampening | `raw_gain / authorized_key_count^0.75` | §1 |
| Identity message re-broadcast | every 30 days | §1 |
| SYNC_REQUEST rate limit | 10 per peer per minute | §1 |
| VDF proof validity | 24 hours from `computed_at` | §10 |
| Tenure decay (consecutive skips) | 1st skip: −1, 2nd: −2, 3rd: −3 (accelerating) | §11 |
| Cold start headcount expiry | 60 days after cold start ends | §8 |
| Storage challenge max per peer per day | 10 | §6 |
| Partition merge tie-break | `from` key (lexicographic, not `id`) | §12 |
| DID_REVOKE effect | immediate, key permanently deauthorized | §1 |
| SHARE re-broadcast interval | 30 minutes | §6 |
| Min rep to replicate | 0.3 | §6 |
| Storage rent base rate | 0.001 per MiB per month | §6 |
| Storage rent billing cycle | 30 days | §6 |
| Storage rent grace period | 7 days (3 days 2nd, 0 days 3rd within 90 days) | §6 |
| Scarcity multiplier formula | 1 + 99 × utilization^4 (cap 100×) | §6 |
| Scarcity multiplier averaging | 24-hour moving average | §6 |
| Storage challenge window size | 4 KiB default, 1–64 KiB range | §6 |
| FLAG details max length | 10 KiB | §6 |
| SHARE max entries per message | 50 | §6 |
| SHARE rate limit | 10 per identity per hour | §6 |
| SHARE tag limits | max 20 tags, max 64 bytes each | §6 |
| Illegal flag deletion threshold | ≥5 flags from ≥3 ASNs, or ≥2 hash-match flags | §6 |
| Illegal flag suspension threshold | ≥3 flags from ≥2 ASNs | §6 |
| Proposed content rent exemption | voting period only, requires ≥3 votes | §6 |
| Provenance adoption split | 70% proposer / 30% original host | §6 |
| Base storage allowance (rep 0.2) | 10 MiB | §6 |
| Min rep to vote | 0.3 | §8 |
| Min rep to flag (dispute) | 0.3 | §6 |
| Min rep to flag (illegal) | 0.5 | §6 |
| FLAG stake (dispute) | -0.005 | §6 |
| FLAG stake (illegal) | -0.02 | §6 |
| False severe flag penalty | -0.05 additional | §6 |
| DID ban threshold | 0.67 (standard governance) | §6 |
| Content chunk size | ≤ 1 MiB per CONTENT_REQUEST | §6 |
| COMMENT rate limit | 1-10/hour (scales with rep), 3/identity/proposal/day | §7 |
| Provenance credit min rep | 0.3 (full rate); below 0.3: 0.002/proposal cap | §6 |
| Rent price locking | at replication time, converges 20%/cycle | §6 |
| Unlinked key voting cooldown | 60 days | §1 |
| Rent multiplier convergence | linear 20%/cycle (full market at 5 cycles; 30%/cycle when divergence >10×) | §6 |
| Storage rent provider share | 80% | §6 |
| Storage rent validator share | 20% | §6 |
| Min challenges per provider per cycle | 10 from ≥2 validators | §6 |
| Provenance credit cap below 0.3 | 0.002 per proposal | §6 |
| CONTENT_WITHDRAW effective delay | 24 hours | §6 |
| RENT_PAYMENT deadline | 7 days into billing cycle | §6 |
| Replication acceptance window | 48 hours | §6 |
| Validator earnings cap | 30 challenges per content per cycle | §6 |
| Validator ASN diversity | ≥2 ASNs for the ≥2 validator requirement | §6 |
| Failed challenge corroboration | required within 24 hours | §6 |
| Accept-and-abandon penalty | -0.002 (challenge fail within 7 days + SHARD_RECEIVED sent) | §6 |
| Rent convergence acceleration | 30%/cycle when divergence >10× | §6 |
| Close-margin confirmation period | 7 days (±0.02 of threshold) | §8 |
| COMMENT per-proposal limit | 3 per identity per rolling 24-hour window | §7 |
| Sync phases | 5 (identity → reputation → proposals → content → storage); 4 and 5 parallel | §5 |
| Sync peer minimum | 5 peers from ≥3 ASNs | §5 |
| Sync jitter (normal) | 0–30 seconds (uniform) | §5 |
| Sync jitter (partition) | 0–300 seconds or min(300, offline_s/100) | §5 |
| Sync backoff | 5s initial, 5m max (exponential) | §5 |
| Sync Merkle agreement | 3-of-5 peers (identity + proposal roots) | §5 |
| Sync bandwidth cap (requester) | 50% of available bandwidth | §5 |
| Sync bandwidth cap (server) | 10 MiB/peer/minute | §5 |
| Sync elevated rate (initial) | 50 SYNC_REQUEST/peer/minute | §5 |
| Incremental sync interval | 15 minutes | §5 |
| Incremental sync identity phase | every 3rd–5th cycle (random, ~45–75 min) + event-triggered on DID_REVOKE | §5 |
| Degraded mode 50% weight | requires phases 1–2 complete, other nodes enforce | §5 |
| Mid-sync stale phase check | 1 hour (re-run phase 1) | §5 |
| Sync failure max retries | 3 before degraded mode (5 min cooldown) | §5 |
| Snapshot publisher min rep | 0.7 | §5 |
| Snapshot max age | 24 hours | §5 |
| Snapshot min publishers | 5 from ≥3 ASNs (+ 2 non-publisher cross-check) | §5 |
| Snapshot post-bootstrap sync | from snapshot_timestamp − 1 hour | §5 |
| Gossip buffer cap (per phase) | 100,000 messages or 100 MiB | §5 |
| Identity Merkle recomputation | batched, at most once per minute | §5 |
| Sync serving uptime credit | max 1/peer/15min, non-empty only, ≥3 distinct peers/day | §5 |
| REPUTATION_CURRENT integrity | trimmed minimum (discard high/low, min of middle 3); fallback on IQR >0.1 | §5 |
| Snapshot publish interval | at least once per 12h, at most once per 6h | §5 |
| VOTE sync_status field | MUST include voter's sync_status at creation time | §5/§8 |
| Gossip buffer priority | DID_REVOKE > KEY_ROTATE > DID_LINK (phase 1); VOTE > PROPOSE > COMMENT (phase 3) | §5 |
| Claims array max | 50 entries, 2 KiB per claim | §7 |
| VDF segment selection | CSPRNG, not derived from proof | §10 |
| Capabilities vocabulary | propose, vote, store (unknown accepted) | §4 |
