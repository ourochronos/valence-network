# v0 — Protocol Specification

v0 is the bootstrap protocol. It is the only version designed by hand — everything after is proposed and ratified by the network through v0's own mechanisms.

## Table of Contents

1. [Identity](#1-identity)
2. [Message Format](#2-message-format)
3. [Transport](#3-transport)
4. [Peer Discovery](#4-peer-discovery)
5. [Gossip](#5-gossip)
6. [Proposals](#6-proposals)
7. [Votes](#7-votes)
8. [Reputation](#8-reputation)
9. [Sybil Resistance](#9-sybil-resistance)
10. [Anti-Gaming](#10-anti-gaming)
11. [Partition Detection](#11-partition-detection)
12. [Protocol Evolution](#12-protocol-evolution)
13. [Error Handling](#13-error-handling)
14. [Open Questions](#14-open-questions)

---

## 1. Identity

A node is an Ed25519 keypair.

- **Private key:** 32 bytes, Ed25519. Never transmitted.
- **Public key:** 32 bytes, Ed25519. Serves as the node's identity.
- **Node ID:** The hex-encoded public key. This is the canonical identifier.

All messages are signed by the sender's private key. All signatures are verified against the sender's public key.

### Key Operations

| Operation | Algorithm | Input | Output |
|---|---|---|---|
| Sign | Ed25519 | message bytes + private key | 64-byte signature |
| Verify | Ed25519 | message bytes + signature + public key | boolean |

### Key Rotation

A node MAY rotate its keypair by publishing a `KEY_ROTATE` message signed by **both** the old and new keys:

```json
{
  "type": "KEY_ROTATE",
  "payload": {
    "old_key": "<hex_old_public_key>",
    "new_key": "<hex_new_public_key>",
    "new_key_signature": "<hex — payload signed by new private key>"
  }
}
```

The envelope itself is signed by the old key (standard signing). The `new_key_signature` field contains the same payload signed by the new private key. Nodes that verify both signatures MUST update their peer table to associate the old key's reputation with the new key.

**Only one rotation per key:** Nodes MUST accept only the **first** KEY_ROTATE message seen for a given `old_key`. Subsequent KEY_ROTATE messages with the same `old_key` but a different `new_key` MUST be rejected. This prevents reputation cloning via race conditions during gossip propagation.

After rotation, messages signed by the old key MUST be rejected (after a **1-hour** grace period to allow propagation). One hour is conservative — gossip convergence in a healthy network is ~15 minutes. The grace period exists for degraded conditions, not normal operation.

When a partition heals and conflicting KEY_ROTATE messages are discovered for the same `old_key` (different `new_key` values), both new keys MUST be suspended — reputation frozen at floor (0.1), voting weight zeroed — until the conflict is resolved via a standard governance proposal that determines the legitimate successor. The protocol adds a KEY_CONFLICT message type for nodes to broadcast discovered conflicts.

**Tenure continuity:** Key rotation transfers reputation but does NOT reset tenure (§10). The new key inherits the old key's full activity history. Tenure tracks the *identity's* history, not the key's.

### Identity Linking

A single operator may run multiple nodes — a local home node, a cloud relay, multiple agents. These are legitimate multi-node deployments, not Sybil attacks. Identity linking lets an operator declare their nodes as a **link group** with one **root key** (the authority) and one or more **child keys**.

#### DID_LINK

```json
{
  "type": "DID_LINK",
  "payload": {
    "root_key": "<hex_public_key>",
    "child_key": "<hex_public_key>",
    "label": "<optional string — human-readable name for the child>"
  }
}
```

The envelope MUST be signed by the root key (`from` = root_key). The payload MUST also include a `child_signature` field: the child key signs `root_key || child_key` to prove possession. Both parties consent to linking.

A root key MAY link multiple children. A child key MUST belong to at most one link group. A child key MUST NOT also be a root key of another group. Link groups are not recursive — flat, one level deep. Hierarchical delegation can be proposed via the protocol.

#### DID_REVOKE

```json
{
  "type": "DID_REVOKE",
  "payload": {
    "root_key": "<hex_public_key>",
    "revoked_key": "<hex_public_key>",
    "reason": "<optional string>"
  }
}
```

Signed by root only. Takes effect immediately — no grace period, no governance vote. The root key is sovereign over its children. After revocation:

- Messages from the revoked key MUST be rejected
- The revoked key's reputation is frozen at floor (0.1)
- The revoked key MAY re-register as a new independent identity (starting at 0.2) but cannot rejoin the link group
- Revocation is permanent and cannot be undone — the revoked key cannot be re-linked to any group

#### Link Group Rules

1. **One vote per proposal per link group.** If multiple keys in a group vote on the same proposal, only the first vote received counts. Subsequent votes from the same group are ignored. The root MAY explicitly delegate voting to a specific child via a `voting_delegate` field in DID_LINK.

2. **Collusion detection exempts linked DIDs.** Vote correlation within a link group is expected and MUST NOT trigger collusion penalties (§10).

3. **ASN diversity counts the group as one operator.** Multiple nodes from the same group across different ASNs count as one operator for ASN diversity calculations (§4). However, the network still benefits from their infrastructure — a group providing nodes across 3 ASNs contributes to network health even though it counts as one operator for diversity scoring.

4. **Tenure is shared across the group.** Activity by any key in the group counts toward the group's tenure (§10). This prevents circumventing tenure penalties by rotating activity across linked identities.

5. **Reputation stays per-DID.** Each key earns and loses reputation independently. A cloud relay node earns storage reputation; a local agent earns proposal reputation. There is no reputation pooling or transfer between linked keys (beyond what KEY_ROTATE provides within a single identity).

6. **Proposal rate limiting is per-group.** The 3-proposals-per-7-days limit (§6) applies to the link group as a whole, not per key.

#### Why Linking Is a Constraint, Not a Benefit

Linking gives up vote multiplication and proposal rate amplification in exchange for not being flagged as a Sybil. An unlinked operator with 3 nodes can cast 3 votes and submit 9 proposals per week — but risks collusion detection. A linked operator casts 1 vote and submits 3 proposals — but is safe from false-positive Sybil flags. Linking is self-policing.

#### Unlinking

A root MAY unlink a child via DID_REVOKE (which also freezes the child). There is no "soft unlink" — separation is revocation. The history of prior linkage is permanent (link and revoke messages are part of the public record). A key that was previously linked and revoked carries that history.

#### Link Group and KEY_ROTATE Interaction

When a root key rotates via KEY_ROTATE (§1), the new root key inherits the entire link group. Children remain linked to the new root. When a child key rotates, it remains in the same link group under the same root.

### DID Representation (Optional)

Nodes MAY represent their identity as a DID for interoperability:

```
did:valence:<hex-encoded-public-key>
```

The protocol does not require DIDs — the raw public key is sufficient.

---

## 2. Message Format

Every message is a signed JSON envelope.

### Canonicalization

All JSON serialization for signing and hashing MUST follow **RFC 8785 (JSON Canonicalization Scheme — JCS)**:

- Object keys sorted lexicographically by Unicode code point
- No whitespace between tokens
- Numbers serialized per ECMAScript rules (no trailing zeros, no leading plus, etc.)
- Unicode: NFC normalization
- Null values included (not omitted)
- Recursively applied to nested objects

Implementations MUST produce identical byte output for identical logical structures. Failure to canonicalize correctly will cause signature verification failures across implementations.

**Numeric types:** All numeric fields in protocol messages MUST be integers. Floating-point values (reputation scores, etc.) MUST be serialized as integers with fixed precision: **multiply by 10,000 and truncate** (e.g., reputation 0.8 → `8000`, threshold 0.67 → `6700`). This eliminates cross-language float serialization divergence. Truncation to fixed-point integers occurs only on storage and wire transmission. Intermediate calculations MUST use at least 64-bit integer arithmetic. When evaluating the reputation formula (§8), operations proceed left-to-right with truncation only on the final result.

### Wire Format

**Stream protocols (point-to-point):** Length-prefixed JSON.

```
+-------------------+-----------------------------+
| 4 bytes (BE u32)  |  JSON payload (UTF-8)       |
| = payload length  |                             |
+-------------------+-----------------------------+
```

Maximum payload size: 8 MiB. Messages exceeding this MUST be rejected.

**GossipSub (broadcast):** Plain UTF-8 JSON. GossipSub handles message boundaries.

### Envelope Schema

```json
{
  "version": 0,
  "type": "<message_type>",
  "id": "<sha256_hex_of_signing_body>",
  "from": "<hex_public_key>",
  "timestamp": <unix_milliseconds>,
  "payload": { ... },
  "signature": "<hex_ed25519_signature>"
}
```

| Field | Type | Description |
|---|---|---|
| `version` | integer | Protocol version. MUST be `0` for this spec. Not included in signing body (see below). |
| `type` | string | Message type. |
| `id` | string | SHA-256 hex digest of the **signing body** (see below). Content address. |
| `from` | string | Hex-encoded Ed25519 public key of the sender. |
| `timestamp` | integer | Unix time in milliseconds when the message was created. |
| `payload` | object | Type-specific payload. |
| `signature` | string | Hex-encoded Ed25519 signature over the signing body bytes. |

### Signing Body

The signature covers the JCS-canonicalized bytes of:

```json
{
  "from": "<from>",
  "payload": <payload>,
  "timestamp": <timestamp>,
  "type": "<type>"
}
```

(Keys in lexicographic order per JCS.)

**Note:** `version` is deliberately excluded from the signing body. This ensures the same logical message has the same `id` and signature regardless of which protocol version it's sent under, preventing message duplication during version transitions.

### Content Addressing

The `id` field is the SHA-256 hex digest of the signing body bytes. Because `id` includes `from` and `timestamp`, two different nodes publishing identical payloads produce different IDs. Messages are deduplicated by `id`.

### Timestamp Validation

Nodes MUST reject messages with timestamps more than **5 minutes** from the node's local time (past or future). This requires loose clock synchronization (NTP). Messages that fail timestamp validation MUST NOT be propagated.

**Clock skew note:** Two nodes both within ±5 minutes of true time can disagree by up to 10 minutes. To prevent this from causing asymmetric message propagation, nodes SHOULD use NTP and SHOULD treat the tolerance as a safety margin, not a target. Nodes with clock skew >2 minutes from the majority of their peers (detected via timestamp comparison in received messages) SHOULD log a warning and attempt clock correction.

### Message Type Inventory

| Type | Transport | Section |
|---|---|---|
| AUTH_CHALLENGE | stream `/valence/auth/1.0.0` | §3 |
| AUTH_RESPONSE | stream `/valence/auth/1.0.0` | §3 |
| PEER_ANNOUNCE | GossipSub `/valence/peers` | §4 |
| PEER_LIST_REQUEST | stream `/valence/sync/1.0.0` | §4 |
| PEER_LIST_RESPONSE | stream `/valence/sync/1.0.0` | §4 |
| SYNC_REQUEST | stream `/valence/sync/1.0.0` | §5 |
| SYNC_RESPONSE | stream `/valence/sync/1.0.0` | §5 |
| REQUEST | GossipSub `/valence/proposals` | §6 |
| PROPOSE | GossipSub `/valence/proposals` | §6 |
| WITHDRAW | GossipSub `/valence/proposals` | §6 |
| ADOPT | GossipSub `/valence/proposals` | §6 |
| VOTE | GossipSub `/valence/votes` | §7 |
| REPUTATION_GOSSIP | GossipSub `/valence/peers` | §8 |
| STORAGE_CHALLENGE | stream `/valence/sync/1.0.0` | §6 |
| STORAGE_PROOF | stream `/valence/sync/1.0.0` | §6 |
| SHARD_QUERY | stream `/valence/sync/1.0.0` | §6 |
| SHARD_QUERY_RESPONSE | stream `/valence/sync/1.0.0` | §6 |
| KEY_ROTATE | GossipSub `/valence/peers` | §1 |
| KEY_CONFLICT | GossipSub `/valence/peers` | §1 |
| DID_LINK | GossipSub `/valence/peers` | §1 |
| DID_REVOKE | GossipSub `/valence/peers` | §1 |

Stream protocol messages (AUTH_*, PEER_LIST_*, SYNC_*, STORAGE_*, SHARD_*) use the envelope format from §2. AUTH_CHALLENGE and AUTH_RESPONSE are exceptions — they use a simplified schema defined in §3 since they occur before identity is established.

---

## 3. Transport

v0 uses libp2p as the transport layer.

### Stream Protocols (Point-to-Point)

| Protocol ID | Purpose |
|---|---|
| `/valence/sync/1.0.0` | Pull-based synchronization and peer exchange |
| `/valence/auth/1.0.0` | Authentication handshake (see below) |

### Authentication Handshake

When two nodes first connect, they exchange an auth challenge over `/valence/auth/1.0.0`:

1. **Initiator** sends `AUTH_CHALLENGE` with a random 32-byte nonce and their own public key
2. **Responder** signs `nonce || initiator_public_key` with their private key and returns `AUTH_RESPONSE`
3. **Initiator** verifies signature (over nonce + own key), VDF proof, and adds peer to table

```json
{"type": "AUTH_CHALLENGE", "payload": {"nonce": "<hex_32_random_bytes>", "initiator_key": "<hex_public_key>"}}
{"type": "AUTH_RESPONSE", "payload": {"signature": "<hex — signs nonce||initiator_key>", "public_key": "<hex>", "vdf_proof": { ... }}}
```

Binding the initiator's key into the signed response prevents replay attacks — an AUTH_RESPONSE is only valid for the specific initiator that issued the challenge.

Nodes MUST complete auth before accepting any other messages from a peer.

### GossipSub Topics (Broadcast)

| Topic | Purpose |
|---|---|
| `/valence/proposals` | New proposals, edits, withdrawals |
| `/valence/votes` | Vote broadcasts |
| `/valence/peers` | Peer discovery announcements |

### NAT Traversal

Nodes SHOULD support libp2p circuit relay v2 for NAT traversal. Nodes with public addresses SHOULD act as relay nodes.

### Connection Security

All connections use libp2p's Noise protocol for encryption and authentication.

---

## 4. Peer Discovery

### Bootstrap

A new node starts with at least one known peer address (a bootstrap node). Bootstrap nodes are regular nodes that happen to be well-known.

Reference implementations MUST include a default bootstrap list. Nodes MAY configure additional or alternative bootstrap nodes.

### Fallback Discovery

When GossipSub-based discovery is unavailable (no connected peers), nodes SHOULD attempt:

1. **mDNS** — discover peers on the local network (for development and LAN deployments)
2. **DNS TXT records** — query a well-known domain for bootstrap peer multiaddrs (e.g., `_valence-bootstrap._tcp.valence.network`)

### Peer Exchange via Sync Protocol

The `/valence/sync/1.0.0` protocol MUST support direct peer list exchange, independent of GossipSub:

```json
{"type": "PEER_LIST_REQUEST", "payload": {"limit": 50, "after": "<node_id | null>"}}
{"type": "PEER_LIST_RESPONSE", "payload": {"peers": [{"node_id": "<hex>", "addresses": ["<multiaddr>"]}], "has_more": true}}
```

Peers are sorted lexicographically by `node_id`. Use `after` for pagination.

This enables a freshly bootstrapped node to discover peers before joining the GossipSub mesh.

### Peer Announcements

Nodes announce on the `/valence/peers` GossipSub topic:

```json
{
  "type": "PEER_ANNOUNCE",
  "payload": {
    "addresses": ["<multiaddr>", ...],
    "capabilities": ["propose", "vote", "store"],
    "version": 0,
    "uptime_seconds": <integer>,
    "vdf_proof": { ... }
  }
}
```

Nodes SHOULD announce every **5 minutes**. Nodes SHOULD prune peers that haven't announced in **30 minutes**.

### Anti-Fragmentation

Nodes MUST periodically connect to random peers outside their immediate neighborhood (suggested: every **10 minutes**). This prevents the network from clustering into disconnected subgraphs.

### ASN Diversity

Nodes MUST track the Autonomous System Number (ASN) distribution of their peers and enforce:

- No single ASN > 25% of connections
- Minimum 4 distinct ASNs (when enough peers are available)

This provides eclipse resistance — an attacker controlling one network segment cannot dominate a node's peer set. ASN diversity is MUST (not SHOULD) because without it, eclipse attacks are trivially cheap (20 nodes across 4 ASNs).

---

## 5. Gossip

### Push

When a node creates or receives a new message, it publishes to the appropriate GossipSub topic.

### Pull (Sync)

Nodes periodically request missed messages from peers via `/valence/sync/1.0.0`:

**Sync Request:**

```json
{
  "type": "SYNC_REQUEST",
  "payload": {
    "since_timestamp": <unix_milliseconds>,
    "since_id": "<message_id — for cursor stability>",
    "types": ["PROPOSE", "VOTE"],
    "limit": 100
  }
}
```

**Sync Response:**

```json
{
  "type": "SYNC_RESPONSE",
  "payload": {
    "messages": [ ... ],
    "has_more": true,
    "next_timestamp": <unix_milliseconds>,
    "next_id": "<message_id>",
    "checkpoint": "<merkle_root_hex>"
  }
}
```

Pagination uses a `(timestamp, id)` cursor to handle multiple messages at the same millisecond deterministically. Sort order: ascending by `timestamp`, then ascending lexicographic by `id`. Messages with `timestamp` > `since_timestamp`, OR (`timestamp` == `since_timestamp` AND `id` > `since_id`), are included.

### Deduplication

Nodes maintain a set of seen message IDs. A message MUST NOT be re-propagated if its `id` has already been seen. Implementations SHOULD use a bounded LRU cache (suggested: **100,000 entries**).

**Time-based rejection (gossip only):** Messages received via GossipSub with timestamps older than **24 hours** MUST be rejected. This prevents replay attacks via cache eviction flooding. This rule does NOT apply to messages received via the `/valence/sync/1.0.0` protocol — sync is explicitly for fetching historical messages (proposals with 14-day voting deadlines, etc.).

---

## 6. Proposals

### Message Types

#### REQUEST

A broadcast need. "I need something."

```json
{
  "type": "REQUEST",
  "payload": {
    "title": "<string>",
    "body": "<string — markdown>",
    "tags": ["<string>", ...],
    "expires": <unix_milliseconds | null>
  }
}
```

#### PROPOSE

A proposed solution. May reference a request, or stand alone.

```json
{
  "type": "PROPOSE",
  "payload": {
    "title": "<string>",
    "body": "<string — markdown description>",
    "content_hash": "<sha256_hex>",
    "content_type": "<mime_type>",
    "content_size": <bytes>,
    "content_url": "<optional — where to fetch the artifact>",
    "content_inline": "<optional — base64, max 1 MiB>",
    "request_id": "<message_id | null — if the referenced request has expired, this proposal still stands on its own>",
    "supersedes": "<message_id | null — previous proposal this replaces>",
    "claims": [
      {
        "statement": "<human-readable claim>",
        "verifiable": <boolean>,
        "evidence": "<optional — URL or description>"
      }
    ],
    "tier": "standard | constitutional",
    "eol": <unix_milliseconds | null>,
    "voting_deadline": <unix_milliseconds>
  }
}
```

**Content delivery:** Small artifacts (< 1 MiB) MAY be inlined as base64 in `content_inline`. Larger artifacts are referenced by `content_hash` and distributed using erasure-coded shards (see below). The hash ensures integrity regardless of source.

**Claims:** Verifiable assertions about the proposal. Signal for evaluation.

**EOL:** Only for protocol change proposals. When the previous protocol version dies.

**Voting deadline:** Required. Proposals MUST specify a deadline (suggested default: 14 days, max: 90 days). Votes received after the deadline MUST NOT be counted.

**Rate limiting:** Nodes are limited to **3 PROPOSE messages per 7-day rolling window**. This prevents spam flooding. Additionally, nodes MUST have reputation ≥ **0.2** (starting rep) to submit proposals — penalized nodes cannot propose until they recover. Nodes SHOULD reject proposals from nodes below starting rep. Proposal rate limits transfer across KEY_ROTATE, like reputation and tenure. The new key inherits the old key's proposal count for the rolling window. For nodes in a link group (§1), the proposal rate limit applies to the group as a whole — 3 proposals per 7-day window across all keys in the group.

#### WITHDRAW

Author withdraws a proposal.

```json
{
  "type": "WITHDRAW",
  "payload": {
    "proposal_id": "<message_id>",
    "reason": "<optional string>"
  }
}
```

Only the original author (verified by `from` key) can withdraw. Nodes MUST stop counting new votes for withdrawn proposals. Nodes that have already locally adopted a proposal are not affected by a later withdrawal.

### Proposal Lifecycle

There is no global state machine. Each node tracks its own view:

- **Open** — proposal exists, votes accumulating, deadline not reached
- **Converging** — weighted endorsement exceeds local threshold
- **Ratified** — node considers the proposal accepted by the network
- **Rejected** — weighted rejection exceeds local threshold, or deadline passed without ratification
- **Adopted** — this specific node has applied the proposal locally and broadcast an ADOPT message
- **Expired** — voting deadline passed
- **Withdrawn** — author withdrew

### Adoption

When a node locally adopts a proposal, it broadcasts an `ADOPT` message:

```json
{
  "type": "ADOPT",
  "payload": {
    "proposal_id": "<message_id>",
    "success": <boolean>,
    "notes": "<optional string>"
  }
}
```

Adoption messages serve as usage attestations — they provide concrete signal that a proposal was applied and whether it worked. They feed into reputation (§8) and help other nodes evaluate proposals.

The `success` field allows nodes to report that they adopted something and it broke — this is valuable negative signal.

**Honest adoption is not provable in v0.** Any node can claim adoption without proof. This is a known limitation. The mitigation is statistical: fabricated adoption messages from colluding nodes will correlate with other collusion indicators (§10). A verifiable adoption mechanism (e.g., proof of execution) can be proposed via the protocol.

### Supersession

Edits to proposals are expressed as new proposals with `supersedes` set to the original proposal's ID. This is simpler than a separate edit mechanism — an edit is just a new proposal that references what it improves. The community votes on the new proposal independently.

### Content Delivery

Large artifacts (> 1 MiB) are distributed using erasure-coded shards across willing peers.

**Erasure coding levels:**

| Level | Data Shards | Parity Shards | Description |
|---|---|---|---|
| minimal | 3 | 2 | 3-of-5 — any 3 shards reconstruct the artifact |
| standard | 5 | 3 | 5-of-8 — default for proposals |
| resilient | 8 | 4 | 8-of-12 — for high-value artifacts |

Protocol change proposals MUST use 'standard' or 'resilient' erasure coding. Nodes SHOULD reject protocol change proposals with 'minimal' coding.

**Flow:**
1. Proposer erasure-codes the artifact into shards
2. Proposer distributes shards to willing peers (nodes with `"store"` capability)
3. Each shard is content-addressed by its own hash
4. The proposal's `content_hash` is the hash of the original artifact
5. Nodes wanting the artifact request shards from peers, reconstruct with any `data_shards` count of shards

**Shard metadata** is included in the proposal:

```json
"content_shards": {
  "coding": "standard",
  "data_shards": 5,
  "parity_shards": 3,
  "shard_hashes": ["<hex>", ...],
  "shard_size": <bytes>,
  "manifest_hash": "<SHA-256 of: shard_hashes sorted lexicographically as hex strings, concatenated without delimiters, then appended with content_hash hex string — all as UTF-8 bytes>"
}
```

Nodes storing shards earn reputation via storage challenges (§8).

**Storage challenges** use adjacency proofs rather than shard retrieval. A challenger picks a random offset within a shard and asks: "What is the hash of the N bytes before/after this offset?" This requires the node to actually hold the shard data — it can't answer by fetching the shard on demand because it doesn't know which offset will be challenged. Challenges are fast (hash a small window) and hard to fake without possessing the data.

```json
{"type": "STORAGE_CHALLENGE", "payload": {"shard_hash": "<hex>", "offset": <bytes>, "direction": "before | after", "window_size": <bytes>, "challenge_nonce": "<hex — random 32 bytes>"}}
{"type": "STORAGE_PROOF", "payload": {"proof_hash": "<SHA256(challenge_nonce || window_bytes)>"}}
```

The challenger verifies the proof against their own copy of the shard. **Challengers MUST be able to independently verify the proof**, either by holding the shard directly or by reconstructing it from sufficient data shards — verification by proxy (asking another node for the answer) is vulnerable to collusion rings where confederates pass each other's challenges without storing anything. Challenges SHOULD be issued randomly by peers who hold the same shard, suggested frequency: once per shard per day.

`content_url` remains as a fallback for HTTP-hosted artifacts.

**Shard discovery:** Nodes that store shards announce this via the sync protocol. When a node requests a proposal's content, it can ask any peer "do you have shards for content_hash X?" via a `SHARD_QUERY` / `SHARD_QUERY_RESPONSE` exchange on `/valence/sync/1.0.0`:

```json
{"type": "SHARD_QUERY", "payload": {"content_hash": "<hex>"}}
{"type": "SHARD_QUERY_RESPONSE", "payload": {"available_shards": [0, 3, 5], "shard_hashes": ["<hex>", ...]}}
```

Nodes MAY also gossip shard availability on `/valence/proposals` when they begin storing shards for a proposal.

**Shard health:** The protocol does not mandate shard replication monitoring in v0. Nodes that store shards SHOULD periodically verify that at least `data_shards + 1` copies of the full shard set remain available across known peers. When replication drops below threshold, nodes SHOULD re-distribute shards to willing peers. A formal shard health protocol can be proposed via the network.

---

## 7. Votes

### VOTE Message

```json
{
  "type": "VOTE",
  "payload": {
    "proposal_id": "<message_id>",
    "stance": "endorse | reject | abstain",
    "reason": "<optional string>"
  }
}
```

### Vote Rules

- One vote per node per proposal. A second vote from the same `from` key supersedes the first.
- Votes are weighted by the voter's reputation (see §8).
- **Abstain** votes count toward participation quorum but do NOT affect the endorse/reject ratio. This lets nodes signal "I showed up but don't have a strong opinion" without being forced to pick a side. Abstention is particularly important for constitutional proposals where participation quorum matters.
- Nodes MAY vote on their own proposals. The weight is the same.
- Votes received after the proposal's `voting_deadline` MUST NOT be counted.

### Local Evaluation

Each node evaluates proposals independently. Nodes MUST re-evaluate proposal status whenever new votes (including superseding votes) are received. There is no global "this passed" event.

A node considers a proposal ratified when **all**:

1. **Minimum voters:** At least **3** distinct nodes have voted (absolute minimum)
2. **Quorum met:** Total reputation weight of all voters ≥ **quorum_weight** (see below)
3. **Threshold met:**
   ```
   weighted_endorse / (weighted_endorse + weighted_reject) >= local_threshold
   ```

Where:
- `weighted_endorse` = sum of `vote_time_reputation` for all endorse votes
- `weighted_reject` = sum of `vote_time_reputation` for all reject votes
- `vote_time_reputation` = the voter's reputation **at the time the vote was created** (the vote message's `timestamp` field), not when it was received. Nodes reconstruct this from the most recent REPUTATION_GOSSIP they hold with an `assessment_timestamp` prior to the vote's timestamp. This reduces vote weight divergence across nodes compared to receive-time snapshots, though nodes with different REPUTATION_GOSSIP histories will still reconstruct slightly different weights. This divergence is bounded and acknowledged in §14.
- `local_threshold` = configured per-node (suggested default: **0.67**)
- Nodes SHOULD use a higher threshold for protocol change proposals (suggested: **0.80**)

**Split ratification:** Because reputation data is gossip-informed and not globally consistent, nodes MAY reach different ratification conclusions for the same proposal. When a node observes ADOPT messages for a proposal it considers not-ratified, it SHOULD re-evaluate using any new reputation data available. If still not ratified locally, the node is not obligated to adopt — local evaluation is authoritative. Persistent disagreement is expected to be rare and self-correcting as reputation data converges.

### Network Phases

The network has two governance thresholds. Both require sustained node count AND can be accelerated by genuine activity.

**Definitions:**
- `active_nodes` = nodes with a PEER_ANNOUNCE received within the last 30 minutes (consistent with peer expiry in §4)
- `total_known_reputation` = sum of locally-computed reputation for all active nodes
- "Sustained" means the node has observed ≥ N active peers (per PEER_ANNOUNCE received within the last 30 minutes) for every consecutive clock-hour in the sustain period. A single clock-hour below threshold resets the sustain clock.

**< 16 nodes:** The protocol is frozen. Content proposals (skills, configs, documents, code) work — that's what the network is for. But **all governance proposals are rejected**. Share, collaborate, build reputation. No rule changes.

**≥ 16 nodes, sustained:** Standard (Tier 1) governance activates. The sustain period depends on network activity:

```
sustain_days = max(3, 7 / activity_multiplier)
```

Where `activity_multiplier` (range 1.0–2.33) is:

```
activity_multiplier = 1.0 + min(1.33, adopted_proposals × 0.33 + earned_rep_delta × 2.0 + challenge_pairs × 0.1)
```

- `adopted_proposals` = number of content proposals with ≥ 3 distinct ADOPT messages from distinct nodes, during the sustain evaluation window
- `earned_rep_delta` = sum of (current_rep - initial_rep) across all active nodes where current_rep > initial_rep, capped at 1.0 for this formula
- `challenge_pairs` = number of distinct (challenger, challenged) node pairs that have completed at least one STORAGE_CHALLENGE/STORAGE_PROOF exchange, capped at 10 for this formula

All inputs are derived from publicly observable protocol messages. The formula is evaluated locally by each node using its own view of the network.

At minimum activity (multiplier 1.0): 7 days. At high activity (multiplier 2.33): 3 days. The floor of 3 days is non-negotiable — gossip convergence and anomaly detection need time.

Activity acceleration does NOT apply to the constitutional threshold.

**≥ 1,024 nodes for 30 consecutive days:** Constitutional (Tier 2) governance activates. No activity acceleration — constitutional governance should be deliberately slow to unlock. The 30-day sustain requirement prevents Sybil-driven threshold gaming.

### Cold Start

For the first **30 days** after standard governance activates, voting uses **headcount mode**: majority of known nodes must vote, >67% endorse. No reputation weighting. This prevents a tiny number of early grinders from dominating governance before reputation has differentiated.

Proposals are evaluated under the voting mode in effect when their `voting_deadline` expires. A proposal submitted during cold start whose deadline falls after cold start ends is evaluated under weighted voting.

However, proposals submitted during cold start MUST additionally require that at least 50% of active nodes have cast a vote (endorse, reject, or abstain), even if evaluated under weighted voting. This prevents gaming the quorum discontinuity at the cold start boundary.

After cold start ends, weighted voting activates:

- Standard quorum: `max(active_nodes × 0.1, total_known_reputation * 0.10)` (scales with network, floor tied to network size)
- Constitutional quorum: `total_known_reputation * 0.30` (30% of the network's reputation must participate)

### Proposal Tiers

Once critical mass is reached, proposals fall into two tiers:

#### Tier 1: Standard

Skills, configs, documents, code, feature proposals. The normal business of the network.

- **Threshold:** 0.67 weighted endorsement
- **Quorum:** standard quorum weight
- **Voting deadline:** 14 days default, 90 days max

#### Tier 2: Constitutional

Changes to the protocol's core safety properties. A proposal is constitutional if it modifies any of:

- The reputation floor
- Sybil resistance mechanisms (VDF or equivalent)
- The voting threshold or quorum formula
- The constitutional tier itself (meta-governance)
- Key rotation / identity mechanisms
- The critical mass threshold

Constitutional proposals require:

- **Threshold:** 0.90 weighted endorsement
- **Quorum:** 30% of total known network reputation (not a multiplier — a percentage of the whole network)
- **Voting deadline:** 90 days minimum
- **Cooling period:** 30 days between ratification and activation (allows nodes to evaluate and leave if they disagree)

The constitutional tier does NOT make anything immutable — the network can still change these properties. It just requires overwhelming consensus, a long deliberation period, and gives dissenting nodes time to exit.

Below 16 nodes, governance is disabled. At 16, standard governance activates. At 1,024, constitutional governance unlocks and all v0 protocol parameters become Tier 2 protected.

---

## 8. Reputation

### Score

Each node maintains a reputation score for every peer it knows about.

| Field | Type | Description |
|---|---|---|
| `overall` | float | 0.0–1.0. **Hard cap.** Reputation cannot exceed 1.0. |
| `by_domain` | map | Domain-specific scores (e.g., `{"skills": 0.7, "config": 0.4}`) |
| `verification_count` | integer | How many times this peer has verified claims |
| `stake_at_risk` | float | Reputation currently at risk from disputed claims |

### Initial Score

New nodes start at **0.2**. This low starting point means:

- **Below 0.2** = penalized. Clear signal of bad behavior.
- **At 0.2** = new, unproven. Default state.
- **0.3–0.5** = contributing, building trust.
- **0.5+** = established contributor.
- **0.8+** = highly trusted veteran.

The low start ensures that abandoning a penalized identity and re-registering is *always* a loss (any rep above 0.2 is forfeited). It also means earned reputation is clearly distinguishable from default reputation.

### Floor

Reputation cannot drop below **0.1**. This prevents permanent exclusion and allows recovery.

### Earning Reputation

| Action | Reward | Notes |
|---|---|---|
| Proposal adopted by peers | +0.005 per ADOPT message received | Capped at +0.05 per proposal |
| Claim verified true | +0.001 | Confirmation reward |
| Claim found false (by you) | +0.005 | Contradiction reward |
| First to find contradiction | +0.01 | First-finder bonus |
| Storage verified via adjacency challenge | +0.001 per challenge passed | Requires knowledge of shard structure (see below) |
| Uptime (continuous availability) | +0.001 per day | Caps at 30 days accumulation |

### Losing Reputation

| Action | Penalty | Notes |
|---|---|---|
| Proposal with false claims | -0.003 per false claim | Contradiction penalty |
| Failed storage challenge | -0.01 | Claimed to store shard but failed adjacency challenge |
| Collusion detected | -0.05 | See §10 |
| Inactivity | -0.02 per month | After 30 days inactive. A node is "active" if it has authored and broadcast at least one signed protocol message (VOTE, PROPOSE, ADOPT, STORAGE_PROOF, or PEER_ANNOUNCE) within the window. PEER_ANNOUNCE alone suffices — it proves liveness without requiring governance participation. |

### Velocity Limits

| Limit | Value | Applies |
|---|---|---|
| Maximum daily gain | 0.02 | Above 0.2 (starting score) |
| Maximum weekly gain | 0.08 | Above 0.2 |

**Recovery below starting score is uncapped.** A node penalized below 0.2 can recover at whatever rate their contributions earn, with no daily/weekly cap, until they reach 0.2 again. Above 0.2, velocity limits apply normally.

This completely eliminates identity recycling: re-registering always gives you 0.2, which is the *minimum* any non-penalized node has. There is never an advantage to abandoning an identity.

### Reputation Propagation

Reputation is **locally computed** and **gossip-informed**.

When Node A evaluates Node B's reputation:

1. Start with A's direct observations of B
2. Query peers for their signed assessments of B
3. Weight each peer's assessment by A's trust in that peer
4. Combine:

```
reputation(B) = α × direct_observations(B) + (1-α) × Σ(trust(P) × assessment_P(B)) / Σ(trust(P))
```

Where:
- `α` = **scales with observation count**: `min(0.6, observation_count / 10)`. With 0 direct observations, α = 0 (pure peer assessment). At 6+ observations, α = 0.6. An observation is any protocol message authored by the subject node (identified by `from` field) that this node has directly received and verified: PROPOSE, VOTE, ADOPT, STORAGE_PROOF, PEER_ANNOUNCE, or REPUTATION_GOSSIP. Deduplicate by message `id` — each unique message counts as one observation. This prevents a "stranger danger" dynamic where unknown nodes are systematically penalized.
- When `α = 0` (no direct observations), `direct_observations(B)` is not used. The node's reputation is entirely peer-informed until direct interaction occurs. When α = 0, the peer-informed reputation MUST be capped at the starting reputation (0.2) until assessments from at least 3 independent assessors from distinct ASNs have been received. This prevents reputation injection via sock puppet attestations.
- Sum is over peers P that have shared signed assessments of B
- `trust(P)` = A's direct reputation score for P (NOT recursive — to avoid circular dependency)

**Evaluation order (fixed-point):** All intermediate values use 64-bit scaled integers (×10,000). The formula evaluates as:
```
peer_sum = Σ(trust_i × assessment_i)     // each already ×10,000, product is ×10^8
peer_weight = Σ(trust_i)                  // ×10,000
peer_avg = peer_sum / peer_weight         // back to ×10,000 scale
result = (α × direct + (10000 - α) × peer_avg) / 10000
```
Truncation occurs only on `result` for storage/transmission.

Trust is **one level deep**: you trust your peers based on your direct experience with them, and their assessments influence your view of nodes you haven't directly observed.

### Signed Reputation Attestations

Reputation gossip MUST be signed. Each assessment is attributable.

```json
{
  "type": "REPUTATION_GOSSIP",
  "payload": {
    "assessments": [
      {
        "node_id": "<hex_public_key>",
        "overall": <float>,
        "by_domain": { ... },
        "observation_count": <integer>,
        "last_observed": <unix_milliseconds>,
        "assessment_timestamp": <unix_milliseconds>
      }
    ]
  }
}
```

The `assessment_timestamp` field records when the assessor last updated this specific assessment. Nodes MUST reject assessments with an `assessment_timestamp` older than the most recent assessment they hold for the same (assessor, subject) pair. This creates a monotonic sequence per assessment relationship. After KEY_ROTATE, the new key's assessments for a given subject MUST have `assessment_timestamp` ≥ the most recent assessment from the old key for the same subject. The monotonicity chain spans key rotation.

Because assessments are signed by the sender, they are **attributable and comparable**. If Node A's assessments of C vary significantly when compared by B and D, this is noted but **not automatically penalized** — reputation is locally computed and legitimately changes over time. Instead, signed attestations serve as an audit trail: patterns of extreme or strategic variation can inform collusion detection (§10) but honest temporal variation is expected.

Nodes SHOULD share reputation gossip every **15 minutes**, covering **10 random peers** per message.

### Trust Model

The protocol has no dedicated trust subsystem. Trust is **derived from existing protocol signals**, not stored or declared.

**Node trust** is reputation — earned through proposals, voting, storage, and verification (this section). It's the only trust score the protocol defines.

**Content trust** is not defined by the protocol. The signals exist — vote margins, voter reputation at vote time, adoption count and success rate, supersession depth, source diversity (ASN distribution of endorsing nodes) — but how a node combines them is a local decision. A node optimizing for safety might weight voter diversity heavily. A node optimizing for speed might trust high-rep endorsements alone. Both are valid.

The protocol MUST make these signals available via gossip and sync (vote records are public, reputation attestations are signed, adoption messages are broadcast). The protocol MUST NOT prescribe a formula for content confidence. Agent autonomy means nodes evaluate content by their own criteria.

This is deliberate: a prescribed trust formula becomes a gaming target. When trust is locally computed from public signals, there is no single function to exploit — an attacker would need to fool each node's independent evaluation.

---

## 9. Sybil Resistance

### Verifiable Delay Function (VDF)

New nodes MUST compute a VDF proof before participating. This makes mass identity creation expensive.

**Parameters:**
- Algorithm: Iterated SHA-256 (v0). Acknowledged limitation: verification requires recomputation. A proper VDF (Wesolowski/Pietrzak) with fast verification can be proposed via the protocol.
- Difficulty: **1,000,000 iterations** (~30 seconds on commodity hardware)
- Input: The node's public key bytes
- Output: VDF proof (hash chain output + intermediate checkpoints)

**Iteration semantics:**
```
h[0] = SHA-256(public_key_bytes)
h[i] = SHA-256(h[i-1])           for i = 1..difficulty
output = h[difficulty]
checkpoint[k] = h[k × 100,000]   for k = 1..10
```

Reference implementations MUST include a test vector suite with known-good checkpoints for the standard difficulty.

**Intermediate checkpoints:** The proof includes hashes at every 100,000th iteration (10 checkpoints). Verifiers MUST verify at least **5 randomly selected segments** (recompute from checkpoint N to checkpoint N+1 and compare). This takes ~15 seconds but provides 99.97% detection rate against single-segment forgery (probability of evading = (5/10)^5 ≈ 3%). Full chain verification (all 10 segments, ~30 seconds) SHOULD be performed for the first peer accepted from each new ASN.

### Registration Flow

1. Generate Ed25519 keypair
2. Compute VDF proof over public key bytes
3. Include proof in auth handshake (§3)
4. Peers verify the proof (spot-check) before accepting the node

### VDF Proof Schema

```json
{
  "output": "<hex>",
  "input_data": "<hex — public key bytes>",
  "difficulty": 1000000,
  "checkpoints": [
    {"iteration": 100000, "hash": "<hex>"},
    {"iteration": 200000, "hash": "<hex>"},
    ...
  ]
}
```

### Rate Limiting

- Maximum 50 VDF verifications per day
- Maximum 5 new peers accepted per hour
- VDF proofs with `input_data` that doesn't match the sender's public key MUST be rejected
- VDF proofs MUST be fresh — verified during the auth handshake, not pre-shared

---

## 10. Anti-Gaming

### Collusion Detection

Nodes SHOULD analyze voting patterns for collusion indicators:

1. **Voting correlation:** Flag groups of 3+ nodes with >95% vote correlation over 20+ proposals
2. **Registration timing:** Flag 3+ nodes whose VDF proofs were computed within a 24-hour window
3. **ASN clustering:** Flag when >25% of active voters share an ASN

Nodes within a declared link group (§1) are exempt from collusion detection. Correlated voting between linked identities is expected.

Detection is local — each node runs its own analysis. When collusion indicators are found:

- **WARNING severity:** Log, reduce trust in flagged nodes
- **HIGH severity:** Reduce reputation of flagged nodes by 0.05
- **Nodes MAY share collusion alerts** via proposals ("I've detected collusion, here's the evidence, should we act?")

### Tenure Limits

A **voting cycle** is a rolling 30-day window. A node is "active in a cycle" if it has **voted on or had proposals ratified** during that window. This tracks participation broadly, not just proposal authorship — a node that votes on everything but never proposes is still accumulating influence.

Nodes active in 6+ consecutive cycles receive a tenure penalty:

- **5% reduction** in vote weight per cycle after the 6th
- This prevents entrenchment — early participants can't permanently dominate
- Tenure tracks the **identity's history**, not the key. Key rotation (§1) transfers tenure along with reputation — rotating keys does NOT reset tenure
- Skipping a cycle **decays** the tenure counter by 1 (not a full reset). To go from tenure 6 to tenure 3, you must skip 3 cycles (90 days of abstention). This makes gaming expensive — a full reset requires 6 months of inactivity, at which point inactivity decay has also reduced your reputation

### Diversity Scoring

When evaluating proposals, nodes MAY weight votes higher when they come from a diverse set of voters (measured by ASN diversity, federation membership, registration age).

---

## 11. Partition Detection

### Merkle Consistency

Nodes periodically compute a Merkle root over their active proposal set.

**Tree construction:**
- **Leaves:** SHA-256 hash of each active (non-withdrawn, non-expired) proposal's `id`
- **Ordering:** Leaves sorted lexicographically by `id`
- **Structure:** Binary Merkle tree, left-biased for odd leaf counts (last leaf promoted)
- **Hash function:** SHA-256 for internal nodes: `SHA256(left_child || right_child)`

When the active proposal set is empty, the Merkle root is the SHA-256 hash of the empty byte string (`e3b0c44298fc1c149afbf4c8996fb924...`).

During sync, nodes exchange Merkle roots in `SYNC_RESPONSE`. Divergent roots trigger a partition event.

### Severity Classification

| Condition | Severity |
|---|---|
| < 5% proposal set difference | info |
| 5–20% difference | warning |
| > 20% difference | critical |

### Proposal Retention

The active proposal set MUST be bounded to prevent unbounded growth. Retention policy:

- **Expired proposals** (past `voting_deadline` without ratification): archived after **7 days**
- **Withdrawn proposals:** archived after **7 days**
- **Rejected proposals** (weighted rejection exceeded threshold): archived after **30 days**
- **Ratified proposals:** archived after **180 days** (6 months)
- **Protocol change proposals:** never auto-archived (permanent record)

Archived proposals are removed from the active set and the Merkle tree. Nodes MAY retain archived proposals locally for historical reference but MUST NOT include them in Merkle root computation or sync responses (unless specifically requested).

**Archival is deterministic, not scheduled.** A proposal is archived the instant it meets the retention criteria above. When computing the Merkle root or responding to sync, implementations MUST exclude any proposal whose archival condition is met at the current time. This ensures all nodes agree on the active set at any given moment without coordinating archival schedules.

### Partition Merge

When partitions heal and nodes discover proposals they missed:

1. Sync all missing proposals and votes from both sides
2. Re-evaluate all proposals with the complete vote set (merged votes may change ratification status)
3. For **content proposals** (skills, configs, docs): contradictory ratifications can coexist. Nodes choose locally.
4. For **protocol change proposals**: contradictory ratifications are resolved by **timestamp priority** — the proposal with the earlier `voting_deadline` takes precedence. If deadlines are equal, the proposal with the lower `id` (lexicographic) wins. Exception: if one proposal's `supersedes` field references the other, the superseding proposal takes precedence regardless of timestamp. The losing proposal is re-opened for voting under the merged network's state.

This is a deterministic tie-breaking rule, not a justice system. It ensures all nodes converge to the same protocol state after a partition heals, which is necessary for the network to function. Content proposals don't need this because they don't affect the protocol itself.

---

## 12. Protocol Evolution

### Self-Hosting

v0 governs its own evolution. Protocol changes are **Tier 2 (constitutional) proposals** once the network reaches critical mass. They require:
- `eol` field set (when the previous version dies)
- 0.90 threshold, 30% network quorum, 90-day minimum voting, 30-day cooling period
- Pre-critical-mass: protocol is frozen, no changes possible

### Version Negotiation

Every message carries `"version": 0`. When v1 is ratified, nodes that adopt it start sending `"version": 1`. During the grace period:

1. Nodes MUST accept messages with version N or N+1
2. Nodes MUST respond in the version the peer sent
3. Nodes MUST cleanly reject (not silently corrupt) messages from unrecognized versions

After EOL, nodes on version N stop accepting messages from version N-1 peers.

### Upgrade Path

A protocol change proposal MUST include:
- The complete new specification (or a diff against the current spec)
- The EOL date for the current version
- A migration path

### EOL Timelines

Set by the community as part of the proposal:
- **Minor changes:** 30 days minimum
- **Major changes:** 90+ days minimum

### Constraints

- Changes MUST NOT cause silent misinterpretation between versions during grace period
- Version negotiation MUST cleanly fail rather than corrupt
- The network can evolve into anything it decides — if it decides to suck, that sucks

---

## 13. Error Handling

### Malformed Messages

Messages that fail parsing, signature verification, or timestamp validation MUST be silently dropped. Nodes MUST NOT propagate invalid messages.

### Unknown Message Types

Messages with unrecognized `type` values SHOULD be propagated but not processed, enabling forward compatibility. However, unknown types are subject to **per-sender rate limiting**: nodes MUST NOT propagate more than **10 unknown-type messages per sender per hour**. Messages exceeding this limit are silently dropped. This prevents DoS via garbage message flooding while preserving forward compatibility for legitimate new message types.

### Unknown Fields

Messages with extra fields beyond the schema MUST be accepted (ignore unknown fields). Implementations MUST NOT reject messages with additional fields. This enables extension without protocol version bumps.

### Missing Proposals

When a node receives a VOTE for a proposal it hasn't seen:

1. Store the vote
2. Request the missing proposal from the vote's sender via sync
3. Apply the vote once the proposal arrives

Votes are valid even if they arrive before their proposal (due to gossip ordering).

### Unresponsive Peers

Nodes SHOULD implement exponential backoff for unresponsive peers, with a maximum retry interval of 10 minutes. After 30 minutes without response, the peer SHOULD be pruned from the active peer table (but not forgotten — it can re-announce).

---

## 14. Open Questions

These are acknowledged gaps in v0. They may be addressed by protocol proposals, by operational experience, or by future spec revisions before v0 is finalized.

### Large-Scale Reputation Gossip

At 10,000 nodes, reputation gossip every 15 minutes at 10 assessments each produces ~111 messages/second. Mitigations to consider:
- Adaptive gossip frequency (less frequent as network grows)
- Only gossip about nodes whose reputation changed
- Hierarchical gossip (cluster-level summaries)

This is a scaling problem that doesn't affect bootstrap. Can be addressed by protocol proposal when the network reaches a size where it matters.

### VDF Hardware Heterogeneity

A 30-second VDF on commodity hardware is 3 seconds on a fast server. The difficulty parameter doesn't equalize across hardware. This means well-resourced attackers face a lower cost per identity. Accepted trade-off for v0: it raises the floor, not the ceiling. A proper VDF (Wesolowski/Pietrzak) with verifiable timing can be proposed via the protocol.

### Blacklisting / Banning

v0 has no mechanism to permanently exclude a node. A node with reputation at floor (0.1) can still participate, just with minimal weight. This is intentional — permanent exclusion is antithetical to the protocol's values. Nodes can locally block specific peers at the implementation level. The network can propose a formal ban mechanism if it decides it needs one.

### Constitutional Participation Incentives

The constitutional quorum (30% of network reputation) combined with explicit abstention (§7) ensures meaningful participation. But turnout may still be low without incentives.

Open design questions:
- **Voting incentives:** Should there be a small reputation reward for voting (including abstaining) on constitutional proposals, to drive turnout? Risk: incentivizing uninformed votes. But without it, participation may rely on civic duty alone.
- **Minimum node-count participation:** In addition to the reputation-weighted quorum, should constitutional proposals require a minimum percentage of *nodes* (not just reputation weight) to vote? This prevents a small number of high-rep nodes from meeting quorum alone.

These need to be resolved before the constitutional tier activates (1,024 nodes), not before v0 ships.

### Hierarchical Delegation

Link groups (§1) are deliberately flat — one root, N children. Hierarchical delegation (root → sub-root → child) introduces complexity: transitive revocation, cascading trust, ambiguous voting authority. It can be proposed via the protocol once there's operational experience with flat link groups.

### Conformance Test Suite

Not yet written. A set of message exchanges with expected outcomes that any implementation must pass. Critical for interoperability. Should be published alongside the first reference implementation.

### Acknowledged Limitations (By Design)

These are inherent properties of the architecture. They are not bugs — they are trade-offs that come with a subjective, gossip-based consensus system:

- **Vote weight divergence:** The same vote has slightly different weight on different nodes due to gossip latency affecting reputation snapshots. This is inherent to combining local reputation with arrival-time snapshots.
- **Information asymmetry:** Well-connected nodes get fresher reputation data, leading to better decisions and a Matthew effect. Mitigated by anti-fragmentation (§4) but not eliminated.
- **Collusion detection evasion:** Sophisticated colluders can add noise to stay below detection thresholds. The detection catches unsophisticated collusion and raises the cost of coordination.
- **Selective gossip:** A well-connected node can selectively relay votes, subtly influencing which proposals get seen. Anti-fragmentation and diverse connections mitigate but don't eliminate this. Vote receipts ("I've seen vote X" attestations) could be proposed as a network service to make selective relay detectable.
- **Bootstrap founder advantage:** Early nodes accumulate reputation and influence first. The adaptive quorum (§7) and tenure penalties (§10) limit this but don't eliminate it. This is the cost of a permissionless bootstrap.

---

## Constants Summary

| Constant | Value | Context |
|---|---|---|
| Ed25519 key size | 32 bytes (private), 32 bytes (public), 64 bytes (signature) | §1 |
| Key rotation grace period | 1 hour | §1 |
| Canonicalization | RFC 8785 (JCS) | §2 |
| Max message payload | 8 MiB | §2 |
| Timestamp tolerance | ±5 minutes | §2 |
| Inline content limit | 1 MiB | §6 |
| GossipSub topics | 3 | §3 |
| Peer announce interval | 5 minutes | §4 |
| Peer expiry | 30 minutes | §4 |
| Anti-fragmentation interval | 10 minutes | §4 |
| Max ASN fraction | 25% | §4 |
| Min distinct ASNs | 4 | §4 |
| Dedup cache size | 100,000 entries (LRU) | §5 |
| Message max age | 24 hours | §5 |
| Voting deadline default | 14 days | §6 |
| Voting deadline max | 90 days | §6 |
| Minimum voters | 3 | §7 |
| Standard governance threshold | 16 nodes sustained 3-7 days (activity-dependent) | §7 |
| Constitutional governance threshold | 1,024 nodes sustained 30 days | §7 |
| < 16 nodes | frozen (content only) | §7 |
| Cold start period | 30 days after governance activation (headcount voting, no early exit) | §7 |
| Standard quorum | max(active_nodes × 0.1, total_rep × 0.10) | §7 |
| Standard threshold | 0.67 | §7 |
| Constitutional threshold | 0.90 | §7 |
| Constitutional quorum | 30% of total network reputation | §7 |
| Constitutional voting minimum | 90 days | §7 |
| Constitutional cooling period | 30 days | §7 |
| Default vote threshold | 0.67 | §7 |
| Proposal tiers | standard (0.67), constitutional (0.90) | §7 |
| Initial reputation | 0.2 | §8 |
| Reputation floor | 0.1 | §8 |
| Max daily reputation gain | 0.02 | §8 |
| Max weekly reputation gain | 0.08 | §8 |
| Monthly inactivity decay | 0.02 | §8 |
| Direct observation weight (α) | min(0.6, observation_count / 10) | §8 |
| Reputation gossip interval | 15 minutes | §8 |
| Reputation gossip batch | 10 random peers | §8 |
| VDF verification segments | 5 minimum (of 10) | §9 |
| VDF difficulty | 1,000,000 iterations | §9 |
| VDF checkpoints | every 100,000 iterations | §9 |
| VDF max verifications/day | 50 | §9 |
| Max new peers/hour | 5 | §9 |
| Vote correlation threshold | 95% over 20+ proposals | §10 |
| Reputation hard cap | 1.0 | §8 |
| Recovery below 0.2 | uncapped (no velocity limit) | §8 |
| Proposal rate limit | 3 per node per 7 days | §6 |
| Min rep to propose | 0.2 (starting rep) | §6 |
| Vote stances | endorse, reject, abstain | §7 |
| Tenure penalty onset | 6 cycles | §10 |
| Tenure penalty factor | 0.95× per cycle | §10 |
| Tenure decay | -1 per skipped cycle (not full reset) | §10 |
| Collusion penalty | -0.05 | §10 |
| Numeric precision | ×10,000 fixed-point integers | §2 |
| Unknown-type rate limit | 10 per sender per hour | §13 |
| Voting cycle | 30-day rolling window | §10 |
| Minor change EOL minimum | 30 days | §12 |
| Major change EOL minimum | 90 days | §12 |
| Expired proposal archive | 7 days | §6 |
| Withdrawn proposal archive | 7 days | §6 |
| Rejected proposal archive | 30 days | §6 |
| Ratified proposal archive | 180 days | §6 |
| Partition merge rule | timestamp priority for protocol proposals | §11 |
| Default erasure coding | standard (5-of-8) | §6 |
| Peer backoff maximum | 10 minutes | §13 |
| Storage challenge nonce | 32 bytes random hex | §6 |
| Assessment timestamp monotonicity | per (assessor, subject) pair | §8 |
| Peer-informed rep cap (α=0) | 0.2 until 3 assessors from distinct ASNs | §8 |
| Protocol proposal min erasure coding | standard or resilient | §6 |
| Empty Merkle root | SHA-256 of empty byte string | §11 |
| Sustained threshold | ≥ N peers every clock-hour in sustain period | §7 |
| Cold start headcount floor | 50% of active nodes for cold-start-submitted proposals | §7 |
| Activity multiplier cap | 2.33 (sustain floor 3 days) | §7 |
| VDF iteration | h[0]=SHA-256(pubkey), h[i]=SHA-256(h[i-1]) | §9 |
| Link group max children | no limit (but one vote per group per proposal) | §1 |
| Revocation effect | immediate, reputation frozen at floor | §1 |
